<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Talking Head Anime from a Single Image 2: More Expressive (Full Version)</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">    
    <link href="css/theme.css" rel="stylesheet">

    <!-- MathJax -->
    <script>
    MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  svg: {
	    fontCache: 'global'
	  }
	};
    </script>
    <script src="mathjax/tex-chtml.js" id="MathJax-script" async></script>    
    <script type="text/javascript" src="js/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="js/bigfoot.min.js"></script>
    <link rel="stylesheet" type="text/css" href="css/bigfoot-default.css">
  </head>

  <body>
  	 <nav class="navbar navbar-expand-md navbar-light fixed-bottom bg-light">
      <div class="collapse navbar-collapse justify-content-center" id="navbarCollapse">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#">&nbsp; &nbsp; Top</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Abstract</a>
          </li>          
          <li class="nav-item">
            <a class="nav-link" href="#background">Background</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#problem-spec">Problem Spec</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#data">Data</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#networks">Networks</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#conclusion">Conclusion</a>
          </li>
        </ul>        
      </div>
    </nav>

    <div class="container" style="max-width: 640px;">
    	<span style="visibility: hidden;">
	      \(
	      \def\sc#1{\dosc#1\csod}
	      \def\dosc#1#2\csod{{\rm #1{\small #2}}}
	      \)
    	</span>
      <h1 align="center">&nbsp;</h1>
      <h1 align="center">&nbsp;</h1>
      <h1 align="center">Talking Head Anime<br>from a Single Image 2:<br>More Expressive</h1>
      <p align="center">
      <a href="http://pkhungurn.github.io/">Pramook Khungurn</a></a>
      </p>
      <h1 align="center">&nbsp;</h1>
    </div>

    <a name="eyecatcher"></a>
    <div align="center">
		<video id="eyecatcher" autoplay muted playsinline loop>
		    <source src="data/eyecatcher.mp4" type="video/mp4">
		</video>
		<br/>
		The characters are corporate/independent virtual YouTubers and their related characters. Images and videos in this article are their fan arts. <a href="#fn_eyecatcher_footnote" rel="footnote">[footnote]</a>
	</div>

	<div class="container" style="max-width: 640px;">
		<div class="footnotes"><ul>
			<li class="footnote" id="fn_eyecatcher_footnote">					
				<p align="left">Most virtual YouTubers are affiliated with <a href="https://www.ichikara.co.jp">Ichikara Inc.</a>, <a href="https://cover-corp.com/">COVER Corp.</a>, <a href="https://www.774.ai/">774 Inc.</a>, <a href="https://twitter.com/noriopro">Noripuro</a>, and <a href="https://www.kmnz.jp/">KMNZ</a>. The rest are independent. Copyrights of the images belong to their respective owners. </p>
			</li>
		</ul></div>			
		<h1>&nbsp;</h1>		

		<a name="abstract"></a>
		<p><b>Abstract.</b> I extended the <a href="https://pkhungurn.github.io/talking-head-anime/">animation-from-a-single-image neural network system I created in 2019</a> so that the characters can make more types of facial expressions. While the old system can only open/close the eyes and the mouth, this new version affords more eye/mouth shapes and can control the eyebrows and the irises. Not counting facial rotations, controllable variations are equivalent to 37 blendshapes <a href="#fn_blend_shapes">[footnote]</a> and 2 bone rotation parameters. They allow a character to show various emotions and give more convincing impression of speech.</p>

		<p>
			<table align="center">
				<tr>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/headshot.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000001.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000002.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px;">	
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000003.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
				</tr>
				<tr>
					<td align="center"><font size="2">Input <a href="#fn_gibara">[copyright]</a></font></td>
					<td align="center"><font size="2">Happy</font></td>
					<td align="center"><font size="2">Sad</font></td>
					<td align="center"><font size="2">Angry</font></td>
				</tr>
				<tr>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000004.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000005.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000006.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">
						</div>
					</td>
					<td>
						<div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">
						<img src="data/characters/otogibara_era/emotion/00000007.png" style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px">	
						</div>
					</td>
				</tr>
				<tr>
					<td align="center"><font size="2">Disgusted</font></td>
					<td align="center"><font size="2">Condescending</font></td>
					<td align="center"><font size="2">Uwamedukai <a href="#fn_uwamedukai">[footnote]</a></font></td>
					<td align="center"><font size="2">Gangimari-Gao <a href="#fn_gangimari">[footnote]</a></font></td>
				</tr>	
			</table>			
		</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_gibara">
					<p align="left">
					The character is <a href="https://www.youtube.com/channel/UCwQ9Uv-m8xkE5PzRc7Bqx3Q">Otogibara Era</a> (&copy; Ichikara Inc.).
					</p>
				</li>
				<li class="footnote" id="fn_blend_shapes">
					<p align="left">In 3D animation, facial expressions are often implemented by interpolating between an expressionless model (aka the "rest" model) and several blendshapes. Here, a <b>blendshape</b> is a separate model that makes a specific facial expression &mdash; for example, having its mouth open &mdash; while otherwise being the same as the base model. See <a href="http://www.cs.cornell.edu/courses/cs4620/2014fa/lectures/21animation-forweb.pdf">this lecture note</a> for more details.</p>
				</li>
				<li class="footnote" id="fn_uwamedukai">
					<p align="left">
					<b>Uwamedukai</b> (上目遣い) is Japanese for the pose where a shorter person looks at another taller one with upturned eyes while tilting the face down. See <a href="https://dic.pixiv.net/a/%E4%B8%8A%E7%9B%AE%E9%81%A3%E3%81%84">this link</a> for more examples.
					</p>
				</li>
				<li class="footnote" id="fn_gangimari">
					<p align="left">
					<b>Gangimari-Gao</b> (ガンギマリ顔) is a facial expression  where a character glares at the viewer with the eyes wide open and the irises reduced in size while smiling. The disconcerting, if not borderline insane, look gives the impression that the character is high on drugs (キマっている). The expression is popularized by virtual YouTuber <a href="https://www.youtube.com/channel/UCqm3BQLlJfvkTsX_hvm0UmA">Tsunomaki Watame</a>. 
					See her in action <a href="https://www.youtube.com/watch?v=zBgQ_7ua_yc">here</a>.
					</p>
				</li>
			</ul>
		</div>

		<p>With the new network, I can drive character illustrations with motions authored for 3D models.</p>

		<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/mfENtYixnNE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>		
		</p>

		<p>I also created a real-time motion transfer tool that provides more controls over the character's face.</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/m13MLXNwdfY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>

		<p>I modified the tool to record my motion and was later able make multiple characters talk and sing with more dynamic lip and face movements.</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/_O5BEcUz3Bw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>
			

		<h1>&nbsp;</h1>
		<h2>1 &nbsp; Introduction</h2>

		<p>The year 2020 saw the rise of the <i>virtual YouTubers</i> (abbreviated as "VTuber" from now on). These are anime characters that are performed in real time by specific actors through the help of computer graphics technology. They contribute contents and stream live performances to various online platforms, with YouTube being where they first gained popularity and thus becoming their namesake. In Japan, VTubers have infiltrated into established entertainment channels. Some <a href="https://www.moguravr.com/agqr-vtuber-program/">hosted radio shows</a> and performed in <a href="https://www.tv-tokyo.co.jp/watanuki/">TV dramas</a> and <a href="https://virtualsan-looking.jp/">anime</a>. NHK, the TV broadcaster funded by the Japanese government, has hosted <a href="http://www6.nhk.or.jp/anime/topics/detail.html?i=5195">several</a> <a href="http://www6.nhk.or.jp/anime/topics/detail.html?i=5922">music</a> <a href="https://www.nhk.jp/p/ts/68XNJ5WZ48/">programs</a> where VTubers took center stage. Globally, YouTube reported significant <a href="https://twitter.com/youtubecreators/status/1338924287504355335">VTuber viewership growth since October 2020</a>. Moreover, the Nikkei recently reported that VTubers have earned the top three places in Superchat revenue worldwide with the top earners making around 100 million yen ($\approx$ 1M USD) in less than 2 years <a href="#fn_nikkei_vtuber">[Nikkei 2020]</a>
		</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_nikkei_vtuber">
					<p align="left"><b>Vチューバー、雑談で1億円　投げ銭世界トップ3独占 [VTuber, earning 100 million yen by chatting and monopolizing the worldwide top 3 places in performer tipping revenue].</b> 17 November 2020. <i>The Nihon Keizai Shimbun.</i> Retrieved from <a href="https://www.nikkei.com/article/DGXMZO66281130W0A111C2TJ1000">https://www.nikkei.com/article/DGXMZO66281130W0A111C2TJ1000</a></p>
				</li>
			</ul>
		</div>

		<p>I personally have been a VTuber fan since 2018. Seeking to combine my fandom with my computer science learnings, I started doing research on character animation with the aim of making it easier to become a VTuber. Noticing that movements of most VTubers are rather simple, I created a system that can animate faces of anime characters in single images in 2019 <a href="#fn_khungurn_2019">[Khungurn 2019]</a>. The system takes as input an image of an anime character looking straight to the viewer and a 6-dimensional pose vector, and it outputs another image of the character with the specified pose. Nevertheless, as noted in the original article, it has many limitations. A major one is that the system only knows how to close the eyes and mouth. On the other hand, characters used professionally can not only deform their eyes and mouths into several different shapes but also move their eyebrows and irises.</p>			

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_khungurn_2019">Pramook Khungurn. 2019. <b>Talking Head Anime from a Single Image.</b> Retrieved from <a href="https://pkhungurn.github.io/talking-head-anime/">https://pkhungurn.github.io/talking-head-anime/</a> </li>
			</ul>
		</div>

		<p>In this article, I address the above shortcoming by proposing a more capable subnetwork that changes the character's facial expression (i.e., a better version of the <a href="https://pkhungurn.github.io/talking-head-anime/index.html#face-morpher">face morpher</a>). While the old face morpher takes only 3 parameters as input, the new one takes 39, and it can move all the movable facial features (eyebrows, eyelids, irises, and mouth) that can be observed in industrial characters. Characters can now express emotions such as happiness, anger, sadness, disgust, and variations between them. Newly afforded mouth shapes allow for better imitation of speech and singing. In Section 7.4, I demonstrate the new capabilities through several <a href="https://en.wikipedia.org/wiki/Vidding">fanvids</a>, including those that show characters being driven by hand-crafted motions and those that transfer human motions to the characters.</p>

		<p>When designing the new face morpher network and constructing datasets to train it, I paid attention to a defining characteristic of drawn characters: layering. When an artist draws a character to be animated, they would separate movable parts into different 2D layers so that they can move the parts independently. Moreover, the order of layers can be anatomically unrealistic: the eyebrows can appear in front of hair bangs that are supposed to cover them. Layering makes drawn characters structurally different from humans because the facial organs are not connected to the surrounding skin.</p>

		<table align="center" cellpadding="5">
			<tr>
				<td align="center">
					<img style="border: 1px solid #000;" src="data/out_of_order_eyebrows/suou_patra.png"><br>
					<a href="https://www.youtube.com/channel/UCeLzT-7b2PBcunJplmWtoDg">Suou Patra</a><br>
					&copy; 774 Inc.
				</td>
				<td align="center">
					<img style="border: 1px solid #000;" src="data/out_of_order_eyebrows/akai_haato.png"><br>
					<a href="https://www.youtube.com/channel/UC1CfXB_kRs3C-zaeTG3oGyg">Akai Haato</a><br>
					&copy; COVER Corp.
				</td>
				<td align="center">
					<img style="border: 1px solid #000;" src="data/out_of_order_eyebrows/ars_almal.png"><br>
					<a href="https://www.youtube.com/channel/UCdpUojq0KWZCN9bxXnZwz5w">Ars Almal</a><br>
					&copy; Ichikara Inc.
				</td>
			</tr>				
		</table>
		<center><b>Figure 1.1</b><a name="figure_1"></a> Characters whose eyebrows appear in front of hair bangs.<br><br></center>

		<p>Taking into account layering and the facial features involved, the new face morpher network deforms the input image in two major steps.
		<ul>
			<li>The first step deals explicitly with the eyebrows. It uses a subnetwork to segment them out and another subnetwork to morph and composite them back to the original image. The segment-then-morph strategy dovetails with the fact that eyebrows are located in their own layers and addresses a difficulty in training networks to deform them: I observed that deformed eyebrows could become blurry unless there was a strong bias to preserve their pixels.</li>

			<li>The second step uses a single subnetwork to deform the other facial features: the irises, the eyelids, and the mouth. It does so in two substeps, reflecting the fact that these features belong to different layers. The first deforms the mouth, the irises, and the sclerae using a combination of appearance flow <a href="#fn_zhou_2016">[Zhou et al. 2016]</a> and partial image change <a href="#fn_pumarola_2018">[Pumarola et al. 2018].</a> The second deforms the eyelids with only a partial image change. As will be shown in Section 7.3, using appearance flow in the first step is important because one needs to preserve high-frequency details of the irises. On the other hand, appearance flow can create artifacts by smearing small features around the eyes, and therefore not using it in the second step leads to better results.</li>
		</ul>
		</p>

		<p>As with my previous article, I generated datasets to train my network by rendering 3D models. However, because 3D renders lack the irregular eyebrow layering discussed above, I generated training examples where the eyebrows were rendered on top of other model parts in order to make sure that my network performs well on drawn characters in the wild.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_pumarola_2018">Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. <b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> ECCV 2018. <a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a></li>
				<li class="footnote" id="fn_zhou_2016">Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. <b>View Synthesis by Appearance Flow.</b> ECCV 2016. <a href="https://arxiv.org/abs/1605.03557">[arXiv]</a></li>
			</ul>
		</div>

		<p>I compare my system to two previous works that can animate existing images. The first is the "Bringing Portraits to Life" paper by Averbuch-Elor et al. <a href="#fn_averbuch_elor_2017">[2017]</a>, which is a representative non-deep-learning technique that animates human photos by moving facial landmarks and deforming the underlying image accordingly. The second is the influential "First Order Motion Model for Image Animation" paper by Siarohin et al. <a href="#fn_siarohin_2019">[2019]</a>, which describes a versatile self-supervised learning algorithm that can transfer motion from a source video to a target image. Siarohin et al.'s paper, in particular, has become well known among Internet users as the technology behind the <a href="https://knowyourmeme.com/memes/dame-da-ne-baka-mitai">Dame Da Ne meme</a>. I show that my system, being designed specifically for anime characters, generate better animations than both previous works. It sensibly deforms the rotated head and yield higher quality facial features than both works, preserving well the iris patterns and crisp lines that are characteristic of anime drawings.</p>

		<div class="footnotes">
			<ul>					
				<li class="footnote" id="fn_averbuch_elor_2017">Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. <b>Bringing Portraits to Life.</b> SIGGRAPH Asia 2017. <a href="http://cs.tau.ac.il/~averbuch1/portraitslife/index.htm">[Project]</a></li>
				<li class="footnote" id="fn_siarohin_2019">Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. <b>First Order Motion Model for Image Animation</b>. NeurIPS 2019. <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">[Project]</a></li>
			</ul>
		</div>

		<a name="background"></a>
		<h2>2 &nbsp; Background</h2>

		<p>The systems proposed in this article and my 2019 article solve the problem of creating talking head animations from a single image, which has been studied extensively by the computer graphics and computer vision community. In this section, I survey previous works with the intention of providing a more comprehensive literature review than what I wrote before. I end the section by recapping the 2019 article.</p>

		<h3>2.1 &nbsp; Related Works</h3>

		<p>A solution to the talking head animation problem takes a <a href="https://en.wikipedia.org/wiki/Head_shot">head shot</a> of a human or a character to be animated, which I shall refer to as the <b>target image</b>. The solution must modify it so that the subject inside is posed according to some specifications. The problem can be classified into two variants based on how pose is specified. In the <b>parameter-based posing</b> problem, a pose is specified explicitly as an array of numerical values which we shall call the <b>pose vector</b>. Here, it is as if the subject is a 3D character model, and the pose vector contains the parameters for controlling its movement. In the <b>motion transfer</b> problem, poses are specified implicitly via an image or a video of another subject (called the <b>source image/video</b>).</p>

		<p>A related problem is the <b>pose estimation</b> problem in which one must produce a pose vector from an image of a subject. Obviously, a pose estimator, paired with a parameter-based poser, can be used to solve the motion transfer problem. Another related problem is the <b>visual dubbing</b> problem where a talking head animation must be generated from a speech recording. I will not survey research on these problems as it is not as related to the core content of this article.</p>

		<h4>2.1.1 &nbsp; Parameter-Based Posing</h4>

		<h5>2.1.1.1 &nbsp; Model-Based Approach</h5>

		<p>A way to pose the target subject is to actually construct a controllable model from the image, pose the model according to the given parameters, and then render it.</p>

		<p><b>3D-based approaches.</b> The seminal work by Blanz and Vetter proposes the first 3D morphable model (3DMM), constructed from a database of 3D face scans, that can be fitted to images of human faces <a href="#fn_blanz_and_vetter">[Blanz and Vetter 1999]</a>. It supports changing facial shapes, and the camera viewpoint can be changed freely when rendering it. However, the eyes and the mouth are not movable, and the model does not include the hair and the torso. While possible changes are limited, similar models can be used to swap faces in photographs <a href="#fn_blanz_2004">[Blanz et al. 2004]</a> and manipulate camera viewpoints <a href="#fn_fried_2016">[Fried et al. 2016]</a>. For facial expression manipulation, Blanz et al. extend the 3DMM to take into account mouth movements and use it to reanimate mouths in photographs and artworks <a href="#fn_blanz_2003">[Blanz et al. 2003]</a>. The first paper that demonstrates manipulation of all facial features through pose vectors is the one by Cao et al.  <a href="#fn_cao_2014">[Cao et al. 2014]</a>, which presents another 3DMM constructed from faces scanned using the <a href="https://en.wikipedia.org/wiki/Kinect">Kinect</a> device. The model is equipped with 46 blendshapes that deform the eyelids and the eyebrows in addition to the mouth, and the authors use it to manipulate the organs in existing photos. Note that the success of all model-based works depends on the quality of the model, and there is a large body of work on 3D face reconstruction from videos and images <a href="#fn_3d_model_reconstruction">[various papers]</a>.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_blanz_and_vetter">Volker Blanz and Thomas Vetter. <b>A Morphable Model For The Synthesis Of 3D Faces.</b> SIGGRAPH 1999. <a href="https://gravis.dmi.unibas.ch/publications/Sigg99/morphmod2.pdf">[PDF]</a></li>
				<li class="footnote" id="fn_blanz_2004">Volker Blanz, Kristina Scherbaum, Thomas Vetter, and Hans-Peter Seidel. <b>Exchanging Faces in Images.</b> Eurographics 2004. <a href="https://gravis.dmi.unibas.ch/publications/EG04.pdf">[PDF]</a></li>
				<li class="footnote" id="fn_fried_2016">Ohad Fried, Eli Shechtman, Dan B Goldman, and Adam Finkelstein. <b>Perspective-aware Manipulation of Portrait Photos.</b> SIGGRAPH 2016. <a href="https://gfx.cs.princeton.edu/pubs/Fried_2016_PMO/fried2016-portraits.pdf">[PDF]</a></li>
				<li class="footnote" id="fn_blanz_2003"> V. Blanz, C. Basso, T. Poggio, and T. Vetter. <b>Reanimating Faces in Images and Video.</b> Eurographics 2003. <a href="http://mi.informatik.uni-siegen.de/publications/blanz_eg03.pdf">[PDF]</a></li>
				<li class="footnote" id="fn_cao_2014">Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. <b>FaceWarehouse: a 3D Facial Expression Database for Visual Computing.</b> IEEE Transactions on Visualization & Computer Graphics, vol. 20, no. 03, pp. 413-425, 2014.</li>
				<li class="footnote" id="fn_3d_model_reconstruction">
					<ol>
						<li>Pia Breuer, Kwang-In Kim, Wolf Kienzle, Bernhard Scholkopf, and Volker Blanz. <b>Automatic 3D Face Reconstruction from Single Images or Video</b>. IEEE International Conference on Automatic Face & Gesture Recognition, Amsterdam, 2008. <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.526.1736&rep=rep1&type=pdf">[PDF]</a></li>

						<li>Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and Steven M. Seitz. <b>Total Moving Face Reconstruction.</b> ECCV 2014. <a href="https://www.supasorn.com/eccv14_totalmoving.pdf">[PDF]</a></li>

						<li>Alexandru Eugen Ichim, Sofien Bouaziz, and Mark Pauly. <b>Dynamic 3D Avatar Creation from Hand-held Video Input.</b> SIGGRAPH 2015. <a href="https://lgg.epfl.ch/publications/2015/AvatarsSG/avatars_sg2015_paper.pdf">[PDF]</a></li>

						<li>Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. <b>What Makes Tom Hanks Look Like Tom Hanks.</b> ICCV 2015. <a href="https://courses.cs.washington.edu/courses/cse455/16wi/notes/Suwajanakorn.pdf">[PDF]</a></li>

						<li>James Booth, Anastasios Roussos, Stefanos Zafeiriou, and Allan Ponniah. <b>A 3D Morphable Model learnt from 10,000 faces.</b> CVPR 2016. <a href="https://ibug.doc.ic.ac.uk/media/uploads/documents/0002.pdf">[PDF]</a></li>

						<li>Joseph Roth, Yiying Tong, and Xiaoming Liu. <b>Adaptive 3D Face Reconstruction from Unconstrained Photo Collections. CVPR 2016. <a href="http://cvlab.cse.msu.edu/pdfs/Roth_Tong_Liu_CVPR16.pdf">[PDF]</a></b></li>

						<li>Marcel Piotraschke and Volker Blanz. <b>Automated 3D Face Reconstruction from Multiple Images using Quality Measures.</b> CVPR 2016. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Piotraschke_Automated_3D_Face_CVPR_2016_paper.pdf">[PDF]</a></li>

						<li>Chenglei Wu, Derek Bradley, Markus Gross, and Thabo Beeler. <b>An Anatomically-Constrained Local Deformation Model for Monocular Face Capture.</b> SIGGRAPH 2016. <a href="https://studios.disneyresearch.com/wp-content/uploads/2019/03/Anatomically-Constrained-Local-Deformation-Model-for-Monocular-Face-Capture.pdf">[PDF]</a></li>		

						<li>Pablo Garrido, Michael Zollhöfer, Dan Casas, and Levi Valgaerts. <b>Reconstruction of Personalized 3D Face Rigs from Monocular Video.</b> ACM Trans. Graph. 35, 3, Article 28 (June 2016), 15 pages. <a href="http://www.zollhoefer.com/papers/SG2016_FaceRig/paper.pdf">[PDF]</a></li>

						<li>Elad Richardson, Matan Sela, and Ron Kimmel. <b>3D Face Reconstruction by Learning from Synthetic Data.</b> 3DV 2016. <a href="https://arxiv.org/abs/1609.04387">[arXiv]</a></li>

						<li>Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. <b>Learning a model of facial shape and expression from 4D scans.</b> SIGGRAPH Asia 2017. <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/400/paper.pdf">[PDF]</a></li>

						<li>Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, Patrick Pérez, and Christian Theobalt. <b>MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction.</b> ICCV 2017. <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Tewari_MoFA_Model-Based_Deep_ICCV_2017_paper.pdf">[PDF]</a></li>

						<li>Matan Sela, Elad Richardson, and Ron Kimmel. <b>Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation. ICCV 2017. </b> <a href="https://arxiv.org/pdf/1703.10131.pdf">[PDF]</a></li>

						<li> Aaron S. Jackson, Adrian Bulat, Vasileios Argyriou and Georgios Tzimiropoulos. <b>Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression.</b> ICCV 2017. <a href="http://aaronsplace.co.uk/papers/jackson2017recon/">[Project]</a></li>

						<li>Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel. <b>Learning Detailed Face Reconstruction from a Single Image.</b> CVPR 2017. <a href="https://arxiv.org/abs/1611.05053">[arXiv]</a></li>

						<li>Luan Tran and Xiaoming Liu. <b>Nonlinear 3D Face Morphable Model.</b> CVPR 2018. <a href="http://cvlab.cse.msu.edu/pdfs/Tran_Liu_CVPR2018.pdf">[PDF]</a></li>

						<li>Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Pérez, Christian Theobalt <b>Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz.</b> CVPR 2018. <a href="https://arxiv.org/abs/1712.02859">[arXiv]</a></li>

						<li>Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, and William T. Freeman. <b>Unsupervised Training for 3D Morphable Model Regression.</b> CVPR 2018. <a href="https://arxiv.org/abs/1806.06098">[arXiv]</a></li>

						<li>Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou. <b>Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network.</b> ECCV 2018. <a href="https://arxiv.org/abs/1803.07835">[arXiv]</a></li>

						<li>James Booth, Anastasios Roussos, Evangelos Ververas, Epameinondas Antonakos, Stylianos Ploumpis, Yannis Panagakis, and Stefanos Zafeiriou. <b>3D Reconstruction of "In-the-Wild" Faces in Images and Videos</b>. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 11, pp. 2638-2652, 1 Nov. 2018. <a href="http://nontas.github.io/publications/files/booth20183d.pdf">[PDF]</a></li>

						<li>Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. <b>Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set.</b> IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG), 2019. <a href="https://arxiv.org/abs/1903.08527">[arXiv]</a></li>

						<li>Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. <b>GANFIT: Generative Adversarial Network Fittingfor High Fidelity 3D Face Reconstruction.</b> CVPR 2019. <a href="https://arxiv.org/pdf/1902.05978.pdf">[PDF]</a></li>

						<li>Luan Tran, Feng Liu, and Xiaoming Liu. <b>Towards High-fidelity Nonlinear 3D Face Morphable Model.</b> CVPR 2019. <a href="http://cvlab.cse.msu.edu/pdfs/Tran_Liu_Liu_CVPR2019.pdf">[PDF]</a></li>

						<li>Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, and Stefanos Zafeiriou. <b>AvatarMe: Realistically Renderable 3D Facial Reconstruction “in-the-wild.”</b> CVPR 2020. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lattas_AvatarMe_Realistically_Renderable_3D_Facial_Reconstruction_In-the-Wild_CVPR_2020_paper.pdf">[PDF]</a></li>

						<li>
							Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, Yu Deng, Yunde Jia, and Xin Tong.
							<b>Deep 3d Portrait from a Single Image.</b>
							CVPR 2020.
							<a href="https://arxiv.org/abs/2004.11598">[arXiv]</a>
						</li>

						<li>Jiangke Lin, Yi Yuan, Tianjia Shao, and Kun Zhou. <b>Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images UsingGraph Convolutional Networks.</b> CVPR 2020. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Lin_Towards_High-Fidelity_3D_Face_Reconstruction_From_In-the-Wild_Images_Using_Graph_CVPR_2020_paper.pdf">[PDF]</a></li>

						<li>Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart. <b>Learning an Animatable Detailed 3D Face Model from In-The-Wild Images.</b> 2020. <a href="https://arxiv.org/abs/2012.04012">[arXiv]</a></li>
					</ol>
					<br>					
				</li>
			</ul>
		</div>

		<p>However, all the above works only process human photos. Papers that deal with non-human characters are scarce. The only one I could find was by Saragih et al. <a href="#fn_saragih_2011">[2011]</a>, which constructs a 3D face model from user-provided landmark annotations and then deforms the model to animate the character. Nevertheless, in addition to requiring manual user intervention, the paper can only animate "masks" as its model only covers the face. Outside the academic world, <a href="https://twitter.com/t_takasaka">高坂</a> has been experimenting with <a href="https://twitter.com/t_takasaka/status/1337722379204657152">fitting 3DMMs to anime-style illustrations</a>. Still, the approach is limited to only the face, and it does not take layering into account.</p>			

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_saragih_2011">Jason M. Saragih, Simon Lucey, and Jeffrey Cohn. <b>Real-time Avatar Animation from a Single Image.</b> IEEE International Conference on Automatic Face & Gesture Recognition and Workshops (FG 2011). <a href="https://www.researchgate.net/publication/224238127_Real-time_Avatar_Animation_from_a_Single_Image">[PDF]</a></li>
			</ul>				
		</div>

		<p><b>2D-based approaches.</b> If the subject to animate is not human, it may be represented by a 2D (or 2.5D) model, which is a collection of image layers that can move independently of one another <a href="#fn_litwinowicz_1991">[Litwinowicz 1991]</a> <a href="#fn_rivers_2010">[Rivers 2010]</a>. The game and VTuber industry have adopted such models, and there are sophisticated proprietary tools for creating them such as <a href="https://www.live2d.com/en/">Live2D</a>, <a href="https://emote.mtwo.co.jp/">E-mote</a>, and <a href="http://esotericsoftware.com/">Spine</a>. <a href="http://yanghuaj.org/">Yanghua Jin</a> of <a href="https://preferred.jp/ja/">Preferred Networks</a> has been working on a system to create 2D models from a single anime drawing. It works by semantically segmenting the given image into layers and assembling them into an E-mote model <a href="#fn_jin_2020">[Jin 2020]</a>. It can be seen in a <a href="https://www.youtube.com/watch?v=SWI5KJvqIfg">promotion video</a> that the generated characters can express a variety of emotions in addition to moving the torso and the whole head. Jin's system's functionality is the most similar to my system's, but the approaches we take are very different.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_litwinowicz_1991">Peter C. Litwinowicz. <b>Inkwell: A 2-D Animation System.</b> SIGGRAPH 1991.</li>
				<li class="footnote" id="fn_rivers_2010">Alec Rivers, Takeo Igarashi, Fredo Durand. <b>2.5D Cartoon Models.</b> SIGGRAPH 2010. <a href="http://www.alecrivers.com/2.5dcartoonmodels/">[Project]</a></li>
				<li class="footnote" id="fn_jin_2020">Yanghua Jin. <b>Crypko - a new workflow for anime character creation.</b> 2020. Retrieved from <a href="https://codh.repo.nii.ac.jp/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=400&item_no=1&page_id=30&block_id=41">[link]</a>.</li>
			</ul>
		</div>

		<h5>2.1.1.2 &nbsp; Image-Based Approach</h5>
		<p>While 3D-based approaches can yield convincing view point changes and geometric deformations, they can only manipulate what the model covers. An alternative approach is to manipulate the target image directly without constructing an explicit model of the subject.</p>

		<p><b>Image translation.</b> When restricted to direct image manipulation, the talking head animation problem becomes a special case of the <b>image translation</b> problem: we are given an image and some (optional) extra information (a pose vector in our case), and we must transform the image according to some criteria. Isola et al. <a href="#fn_isola_2017">[2017]</a> present a general framework for image translation through the use of conditional generative adversarial networks (cGAN) <a href="#fn_mirza_2014">[Mirza and Osindero 2014]</a>. Researchers have then improved the framework by allowing training by unpaired data <a href="#fn_unpaired_image_translation">[several papers]</a> and enabling one network to change multiple discrete image attributes <a href="#fn_stargan">[several papers]</a>.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_isola_2017">Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. <b>Image-to-Image Translation with Conditional Adversarial Networks.</b> CVPR 2017. <a href="https://phillipi.github.io/pix2pix/">[Project]</a></li>
				<li class="footnote" id="fn_mirza_2014">Mehdi Mirza and Simon Osindero. <b>Conditional Generative Adversarial Nets.</b> 2014. <a href="https://arxiv.org/abs/1411.1784">[arXiv]</a></li>
				<li class="footnote" id="fn_unpaired_image_translation">
					<ol>
						<li id="fn_zhu_2017">
			                Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros.
			                <b>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.</b>
			                ICCV 2017.
			                <a href="https://junyanz.github.io/CycleGAN/">[Project]</a>
			                <a href="https://arxiv.org/abs/1703.10593">[arXiv]</a>
			            </li>
			            <li>
			            	Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong.
			            	<b>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation.</b>
			            	ICCV 2017.
			            	<a href="https://arxiv.org/abs/1704.02510">[arXiv]</a>
			            </li>
			            <li id="fn_kim_2017">
			            	Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim.
			            	<b>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.</b>
			            	ICML 2017.
			            	<a href="https://arxiv.org/abs/1703.05192">[arXiv]</a>
			            </li>
			            <li>
			            	Ming-Yu Liu, Thomas Breuel, and Jan Kautz.
			            	<b>Unsupervised Image-to-Image Translation Networks.</b>
			            	NIPS 2017.
			            	<a href="https://arxiv.org/abs/1703.00848">[arXiv]</a>
			            </li>
			            <li id="fn_huang_2018">
			                Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
			                <b>Multimodal Unsupervised Image-to-Image Translation.</b>
			                ECCV 2018.
			                <a href="https://arxiv.org/abs/1804.04732">[arXiv]</a>
	            		</li>
	            		<li id="fn_nizan_2020">
	            			Ori Nizan and Ayellat Tal.
	            			<b>Breaking the cycle - Colleagues are all you need.</b>
	            			CVPR 2020.
	            			<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Nizan_Breaking_the_Cycle_-_Colleagues_Are_All_You_Need_CVPR_2020_paper.pdf">[PDF]</a>
	            		</li>	
					</ol>
				</li>
				<li class="footnote" id="fn_stargan">
					<ol>
						<li class="footnote" id="fn_choi_2018">
			                Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo
			                <b>StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.</b>
			                CVPR 2018.
	                		<a href="https://arxiv.org/abs/1711.09020">[arXiv]</a>	
	                	</li>
	                	<li class="footnote" id="fn_choi_2020">
	                		Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
	                		<b>StarGAN v2: Diverse Image Synthesis for Multiple Domains.</b>
	                		CVPR 2020.
	                		<a href="https://arxiv.org/abs/1912.01865">[arXiv]</a>
	                	</li>
					</ol>
				</li>
			</ul>
		</div>

		<p><b>Facial expression manipulation.</b> While the above papers are not specifically about manipulating faces, they are building blocks that Pumarola et al. use to create a network that modifies human facial features given a specification of facial expression in the form of the <a href="https://en.wikipedia.org/wiki/Facial_Action_Coding_System">Action Units</a> (AUs) encoding <a href="#fn_pumarola_2018_2">[Pumarola et al. 2018]</a>. My system uses their maneuver to partially modify an image (called the "attention-based generator" in the paper) at multiple places. Ververas and Zafeiriou propose another network that achieves the same functionality but uses blendshape weights to specify expression instead of the AUs <a href="#fn_ververas_2020">[2020]</a>. My system also uses blendshape weights to encode facial expression, but the blendshapes are common ones found in multiple models rather than those that come with a 3DMM. The work by Yeh et al. <a href="#fn_yeh_2016">[Yeh et al. 2016]</a> manipulates facial expression by using a variational auto encoder (VAE) <a href="#fn_kingma_2014">[Kingma and Welling 2014]</a> to encode the expression of the target image. The latent code is then manipulated with simple arithmetic and then decoded to get a face with modified expression. Outside academia, <a href="https://www.algoage.net/">AlgoAge</a>, a Japanese AI startup, is commercializing a service called <a href="https://lp.deepanime.com/">DeepAnime</a> which can open and close the eyes and mouth of anime characters given in single images. The system in this article can achieve what DeepAnime does and yields more expressive outputs. <a href="https://zizai.co.jp/">Zizai</a>, a client of AlgoAge, operates a streaming-as-character service called <a href="https://www.live.iriam.com/">Iriam</a> in which a user can upload a character image, and the service would generate from it an avatar with closeable eyes and mouth.</p>

		<div class="footnotes">
			<ul>					
				<li class="footnote" id="fn_pumarola_2018_2">
	                Albert Pumarola, Antonio Agudo, Aleix M. Martínez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
	                <b>GANimation: Anatomically-Aware Facial Animation from a Single Image.</b>
	                ECCV 2018. <a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a> <a href="https://arxiv.org/abs/1807.09251">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_ververas_2020">
	            	Evangelos Ververas and Stefanos Zafeiriou.
	            	<b>SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters.</b>
	            	Int J Comput Vis 128, 2629–2650 (2020). 
	            	<a href="https://link.springer.com/article/10.1007/s11263-020-01338-7">[Paper]</a>
	            </li>
	            <li class="footnote" id="fn_yeh_2016">
	            	Raymond Yeh, Ziwei Liu, Dan B Goldman, and Aseem Agarwala.
	            	<b>Semantic Facial Expression Editing using Autoencoded Flow.</b>
	            	2016.
	            	<a href="https://arxiv.org/abs/1611.09961">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_kingma_2014">
	                Diederik P Kingma and Max Welling.
	                <b>Auto-Encoding Variational Bayes.</b>
	                ICLR 2014.
	                <a href="https://arxiv.org/abs/1312.6114">[arXiv]</a>
	            </li>
			</ul>
		</div>

		<p><b>Face rotation.</b> Another aspect of talking head animation generation is to rotate the face, which is a special case of the problem of rotating objects in single images. Tartarchenko et al. propose an image translator that generate images of rotated objects from scratch <a href="#fn_tatarchenko_2016_1">[Tatarchenko et al. 2016]</a>, but its outputs are blurry due to a combination of direct pixel generation and the L2 loss function used. Zhou et al. improve the result's sharpness by using appearance flow, a map which tells for each pixel of the output which input image pixel to copy from <a href="#fn_zhou_2016_2">[Zhou et al. 2016]</a>. Appearance flow is used in my system not only to rotate the face but also to change facial expressions. Building on Zhou et al.'s work, Park et al. propose a more sophisticated algorithm that identifies which pixels generated by appearance flow are good and fixes bad pixels by hallucinating them <a href="#fn_park_2017_2">[Park et al. 2017]</a>. The <a href="https://pkhungurn.github.io/talking-head-anime/index.html#face-rotator">face rotator</a> in my previous work also fixes bad pixels, but it does so by combining the result of appearance flow with that of direct pixel generation. Zhao et al. propose an algorithm where a rotated human clothed human body is generated in a coarse-to-fine manner in which a VAE generates a coarse image, and a cGAN later refines it <a href="#fn_zhao_2018">[Zhao et al. 2018]</a>. Wiles et al. present X2Face, a self-supervised network that can manipulate the target face according to multiple types of pose inputs, including source videos/images and pose vectors <a href="#fn_wiles_2018">[Wiles et al. 2018]</a>. They demonstrate automatic learning from unlabeled videos of pose vectors that can be used to rotate the head. Zhou et al. give another self-supervised learning algorithm to train a face rotator from unlabeled images, using a 3DMM to generate intermediate synthetic data <a href="#fn_zhou_2020">[Zhou et al. 2020]</a>. Lastly, Wang et al.'s network converts the target image to a self-supervisedly learned 3D feature embedding that can later be rotated and rendered to obtain an output image in which the subject head and body are rotated by small angles <a href="#fn_wang_2020">[Wang et al. 2020]</a>.</p>

		<div class="foonotes">
	        <ul>
	            <li class="footnote" id="fn_tatarchenko_2016_1">
	                Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox.
	                <b>Multi-view 3D Models from Single Images with a Convolutional Network.</b>
	                ECCV 2016.
	                <a href="https://arxiv.org/abs/1511.06702">[arXiv]</a>
	            </li>
	            <li class='foonote' id="fn_zhou_2016_2">
	                Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik,  and Alexei A. Efros.
	                <b>View Synthesis by Appearance Flow.</b>
	                ECCV 2016.
	                <a href="https://arxiv.org/abs/1605.03557">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_park_2017_2">
	                <p align="left">
	                    Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, and Alexander C. Berg.
	                    <b>Transformation-Grounded Image Generation Network for Novel 3D View Synthesis.</b>
	                    CVPR 2017. <a href="https://arxiv.org/abs/1703.02921">[arXiv]</a>
	                </p>
	            </li>
	            <li class="footnote" id="fn_zhao_2018">
	            	Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Zequn Jie, and Jiashi Feng.
	            	<b>Multi-View Image Generation from a Single-View.</b>
	            	MM 2018.
	            	<a href="https://dl.acm.org/doi/pdf/10.1145/3240508.3240536">[PDF]</a>
	            </li>		            
	            <li class="footnote" id="fn_wiles_2018">
	            	Olivia Wiles, A. Sophia Koepke, Andrew Zisserman.
	            	<b>X2Face: A network for controlling face generation by using images, audio, and pose codes.</b>
	            	ECCV 2018.
	            	<a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html">[Project]</a>
	            </li>
	            <li class="footnote" id="fn_zhou_2020">
	            	Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, and Xiaogang Wang.
	            	<b>Rotate-and-Render: Unsupervised Photorealistic Face Rotation from Single-View Images.</b>
	            	CVPR 2020.
	            	<a href="https://arxiv.org/pdf/2003.08124.pdf">[PDF]</a>
	            </li>
	            <li class="footnote" id="fn_wang_2020">
	            	 Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu.
	            	<b>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.</b>
	            	2020.
	            	<a href="https://nvlabs.github.io/face-vid2vid/">[Project]</a>
	            </li>
	        </ul>
	    </div>

		<h4>2.1.2 &nbsp; Motion Transfer</h4>

		<p>Recall that the motion transfer problem deals with the case where the poses that we want the target subject to take are specified implicitly by images or videos of another subject. Here, while there is no need to explicitly extract poses from source images, one must be careful to transfer only the pose and not other information (e.g., appearance and shape) to the target image.</p>

		<h5>2.1.2.1 &nbsp; Model-based Approach</h5>

		<p>The key to this approach is to use a 3DMM with two types of controllable parameters: those that control the identity of the face and those that control deformation of facial organs. Motion transfer then can be achieved by fitting the model to both the source and target subjects and then copy the former's pose parameters to the latter. Vlasic et al. first achieved motion transfer using a 3DMM created from 3D face scans <a href="#fn_vlastic_2005">[Vlasic et al. 2005]</a>. Several later works such as Cao et al.'s <a href="#fn_cao_2014_2">[2014]</a>  and Suwajanakorn et al.'s <a href="#fn_suwajanakorn_2015">[2015]</a>, which propose new ways to create 3DMMs, also show motion transfer as an application of their models. Garrido et al. presented a system specialized to transfer mouth movement of a dubbing actor to another target subject in a video, with its novelty being the use of audio analysis to improve the transferred motion in terms of visual-audio synchronization <a href="#fn_garrido_2016">[Garrido et al. 2016]</a>. Thies et al.'s work requires a target video instead of a target image as input, and it can pose the target subject more naturally by leveraging poses extracted from the target video <a href="#fn_thies_2016">[Thies et al. 2016]</a>. Olszewski et al. use a GAN to generate per-frame texture for the 3DMM to increase realism <a href="#fn_olszewski_2017">[2017]</a>. Kim et al. render a video of the fitted model and then use a space-time cGAN to translate the video to the final output <a href="#fn_kim_2018">[Kim et al. 2018].</a> Poursaeed et al. train a network to morph a 2D character model given by the user to take the poses of the same character in input images. The model parameters can then be interpolated to generate in-between frames  <a href="#fn_poursaeed_2019">[2019]</a>.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_vlastic_2005">
					Daniel Vlasic, Matthew Brand, Hanspeter Pfister, and Jovan Popovic.
					<b>Face Transfer with Multilinear Models.</b>
					SIGGRAPH 2005.
					<a href="https://people.csail.mit.edu/drdaniel/research/vlasic-2005-ftm.pdf">[PDF]</a>
				</li>
				<li class="footnote" id="fn_cao_2014_2">
					Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. 
					<b>FaceWarehouse: a 3D Facial Expression Database for Visual Computing.</b> 
					IEEE Transactions on Visualization & Computer Graphics, vol. 20, no. 03, pp. 413-425, 2014.</li>
				<li class="footnote" id="fn_suwajanakorn_2015">
					Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 
					<b>What Makes Tom Hanks Look Like Tom Hanks.</b> ICCV 2015. 
					<a href="https://courses.cs.washington.edu/courses/cse455/16wi/notes/Suwajanakorn.pdf">[PDF]</a>
				</li>
				<li class="footnote" id="fn_garrido_2016">
					P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi, P. Pérez, and C. Theobalt.
					<b>VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track.</b>
					Eurographics 2016.
					<a href="https://gvv.mpi-inf.mpg.de/files/EuroGraphics2015/dubbing_high.pdf">[PDF]</a>
				</li>
				<li class="footnote" id="fn_thies_2016">
					Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner.
					<b>Face2Face: Real-time Face Capture and Reenactment of RGB Videos.</b>
					CVPR 2016.
					<a href="https://arxiv.org/abs/2007.14808">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_olszewski_2017">
					Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, SitaoXiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li.
					<b>Realistic Dynamic Facial Textures from a Single Image using GANs</b>
					ICCV 2017.
					<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Olszewski_Realistic_Dynamic_Facial_ICCV_2017_paper.pdf">[PDF]</a>
				</li>
				<li class="footnote" id="fn_kim_2018">
					Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, Christian Theobalt.
					<b>Deep Video Portraits.</b>
					SIGGRAPH 2018.
					<a href="https://gvv.mpi-inf.mpg.de/projects/DeepVideoPortraits/">[Project]</a>
				</li>
				<li class="footnote" id="fn_poursaeed_2019">
	                Omid Poursaeed, Vladimir G. Kim, Eli Shechtman, Jun Saito, and Serge Belongie.
	                <b>Neural Puppet: Generative Layered Cartoon Characters</b>
	                <a href="https://arxiv.org/abs/1910.02060">[arXiv]</a>
	            </li>
			</ul>
		</div>

		<h5>2.1.2.2 &nbsp; Keypoint-based Approach</h5>

		<p>One way to ensure that appearance is not transferred from the source to target image is to represent poses with a discrete number of keypoints, typically detected facial landmarks. Keypoints are computed for both the source and target images. The target keypoints are then replaced by the source keypoints or are perturbed, typically by the difference between the source keypoints of the first and current video frames. Lastly, the replaced/perturbed keypoints are used to deform the target image in some way.</p>

		<p>The work by Averbuch-Elor et al. follows this framework, and it deforms the target image by a "lightweight" 2D warp <a href="#fn_averbuch_elor_2017_2">[Averbuch-Elor et al. 2017]</a>. This involves triangulating the target image with the keypoints as vertices and then warp the image by simply moving the keypoints. Geng et al. later improve Averbuch-Elor's algorithm by using a cGAN to add facial details after lightweight 2D warping <a href="#fn_geng_2018">[Geng et al. 2018]</a>. Later works use more sophisticated methods to warp the target image. Song et al. <a href="#fn_song_2017">[2017]</a> use a cGAN. Zakharov et al. turn the target image into latent codes that are used to modulate another network that processes a rendering of landmarks <a href="#fn_zakharov_2019">[2019]</a>. Hao et al. do the exact opposite: landmarks are turned into latent codes that modulates a network that processes the target image <a href="#fn_hao_2020">[2020]</a>. Wang et al. study the problem video-to-video synthesis, a generalization of image translation to videos <a href="#fn_wang_2018">[2018]</a>. An improved algorithm that requires fewer frames from the target video can be used to transfer facial motion given a video of detected landmarks of the source subject <a href="#fn_wang_2019">[2019]</a>.</p>

		<div class="footnotes">
			<ul>					
				<li class="footnote" id="fn_averbuch_elor_2017_2">
					Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. 
					<b>Bringing Portraits to Life.</b> 
					SIGGRAPH Asia 2017. 
					<a href="http://cs.tau.ac.il/~averbuch1/portraitslife/index.htm">[Project]</a>
				</li>
				<li class="footnote" id="fn_siarohin_2019">
					Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 
					<b>First Order Motion Model for Image Animation</b>. 
					NeurIPS 2019. 
					<a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">[Project]</a>
				</li>
				<li class="footnote" id="fn_geng_2018">
					Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and Kun Zhou.
					<b>Warp-Guided GANs for Single-Photo Facial Animation.</b>
					ACM Transactions on Graphics, 37 (6), 2018.						
					<a href="http://eprints.whiterose.ac.uk/138578/1/wgGAN.pdf">[PDF]</a>
				</li>
				<li class="foonote" id="fn_song_2017">
					Lingxiao Song, Zhihe Lu, Ran He, Zhenan Sun, and Tieniu Tan.
					<b>Geometry Guided Adversarial Facial Expression Synthesis.</b>
					2017.
					<a href="https://arxiv.org/abs/1712.03474">[PDF]</a>
				</li>
				<li class="footnote" id="fn_hao_2020">
					Hanxiang Hao, Sriram Baireddy, Amy R. Reibman, and Edward J. Delp.
					<b>FaR-GAN for One-Shot Face Reenactment.</b>
					AI for content creation workshop at CVPR 2020.						
					<a href="https://arxiv.org/abs/2005.06402">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_zakharov_2019">
					Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky.
					<b>Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.</b>
					2019.
					<a href="https://arxiv.org/abs/1905.08233">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_wang_2018">
					 Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro.
					 <b>Video-to-Video Synthesis.</b>
					 NueRIPS 2018.
					 <a href="https://tcwang0509.github.io/vid2vid/">[Project]</a>
				</li>
				<li class="footnote" id="fn_wang_2019">
					 Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro.
					<b>Few-shot Video-to-Video Synthesis.</b>
					NeuRIPS 2019.
					<a href="https://nvlabs.github.io/few-shot-vid2vid/">[Project]</a>
				</li>
				
			</ul>
		</div>			

		<p>The above works use facial landmarks detected by predefined networks. However, landmarks can also be learned with self supervision <a href="#fn_landmark_discovery">[various papers]</a>. In 2018, Siarohin et al. proposed a pose transfer network that uses landmarks learned this way <a href="#fn_siarohin_2018">[Siarohin et al. 2018]</a>. A year later, the same set of authors improved it by adding local affine transformation to each landmark <a href="#fn_siarohin_2019">[Siarohin et al. 2019]</a>. Wang et al. extend Siarohin et al.'s work to use 3D keypoints instead of 2D ones <a href="#fn_wang_2020_2">[Wang et al. 2020]</a>. The method allows them to not only change facial expression but also the rotate the head and the torso (as mentioned in Section 2.1.1.2).</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_landmark_discovery">
					<ol>
						<li id="fn_jakab_2018">
							Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi.
							<b>Unsupervised Learning of Object Landmarks through Conditional Image Generation.</b>
							NeuRIPS 2018.
							<a href="https://proceedings.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf">[PDF]</a>
						</li>
						<li id="fn_zhang_2018">
							Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, and Honglak Lee.
							<b>Unsupervised Discovery of Object Landmarks as Structural Representations.</b>
							CVPR 2018.
							<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf">[PDF]</a>
						</li>
						<li id="fn_change_2020">
							Zezhou Cheng, Jong-Chyi Su, and Subhransu Maji.
							<b>Unsupervised Discovery of Object Landmarks via Contrastive Learning.</b>
							2020.
							<a href="https://people.cs.umass.edu/~zezhoucheng/contrastive_landmark/">[Project]</a>
						</li>							
					</ol>
				</li>
				<li id="fn_siarohin_2018">
					Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.
					<b>Animating Arbitrary Objects via Deep Motion Transfer.</b>
					CVPR 2018.
					<a href="https://arxiv.org/abs/1812.08861">[arXiv]</a>
				</li>		
				<li class="footnote" id="fn_wang_2020_2">
	            	 Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu.
	            	<b>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing.</b>
	            	2020.
	            	<a href="https://nvlabs.github.io/face-vid2vid/">[Project]</a>
	            </li>			
			</ul>			
		</div>

		<p>While landmark positions can represent poses, they also convey the face's shape. Hence, the target subject's identity might be compromised if the source landmarks are used without proper processing. Nevertheless, none of the papers I mentioned in this section seem to address this problem. Recent works by Qiao et al. <a href="#fn_qiao_2018">[2018]</a>, Wu et al. <a href="#fn_wu_2018">[2018]</a>, Xiang et al. <a href="#fn_xiang_2020">[2020]</a>, Zhang et al. <a href="#fn_zhang_2020">[2020]</a>, and Ha et al. <a href="#fn_ha_2020">[2020]</a> propose ways to alleviate it.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_qiao_2018">
					Fengchun Qiao, Naiming Yao, Zirui Jiao, Zhihao Li, Hui Chen, and Hongan Wang.
					<b>Geometry-Contrastive GAN for Facial Expression Transfer</b>
					2018.
					<a href="https://arxiv.org/abs/1802.01822">[PDF]</a>
				</li>
				<li class="footnote" id="fn_wu_2018">
					Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy.
					<b>ReenactGAN: Learning to Reenact Faces via Boundary Transfer.</b>
					ECCV 2018.
					<a href="https://arxiv.org/abs/1807.11079">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_zhang_2020">
					Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, and Changjie Fan.
					<b>FReeNet: Multi-Identity Face Reenactment.</b>
					CVPR 2020.
					<a href="https://arxiv.org/abs/1905.11805">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_xiang_2020">
					Sitao Xiang, Yuming Gu, Pengda Xiang, Mingming He, Koki Nagano, Haiwei Chen, and Hao Li.
					<b>One-Shot Identity-Preserving Portrait Reenactment.</b>
					2020.
					<a href="https://arxiv.org/abs/2004.12452">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_ha_2020">
					Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, and Dongyoung Kim.
					<b>MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets.</b>
					AAAI 2020.
					<a href="https://arxiv.org/abs/1911.08139">[arXiv]</a>
				</li>
			</ul>
		</div>

		<p>I found two works that apply the keypoint-based approach to motion transfer of non-photorealistic characters. Hamada et al. trained a conditional GAN that can generate full-body images of anime characters conditioned on stick figure images of the character's skeleton <a href="#fn_hamada_2018">[2018]</a>. <a href="https://twitter.com/kvfrans">Kevin Frans</a> showed a system that can transfer motion of an anime character to another character generated by the <a href="https://waifulabs.com">Waifu Labs</a> service. From <a href="https://twitter.com/kvfrans/status/1165425991138738176">the thread</a>, it seems the system takes facial landmarks as input. These two works, however, require that the character to be animated be generated by GANs, and so their applicability is limited.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_hamada_2018">
	                Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, and Yusuke Uchida.
	                <b>Full-body High-resolution Anime Generation with Progressive Structure-conditional Generative Adversarial Networks.</b>
	                ECCV Workshop on Computer Vision for Fashion, Art, and Design, 2018.
	                <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Hamada_Full-body_High-resolution_Anime_Generation_with_Progressive_Structure-conditional_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf">[PDF]</a>
	            </li>
			</ul>
		</div>

		<blockquote class="twitter-tweet"><p align="center" lang="en" dir="ltr">one step closer to AI animation!!! <a href="https://t.co/YbLbTRx2JO">pic.twitter.com/YbLbTRx2JO</a></p>&mdash; kvfrans | Skyseeker (@kvfrans) <a href="https://twitter.com/kvfrans/status/1165053750202888192?ref_src=twsrc%5Etfw">August 24, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

		<h5>2.1.2.3 &nbsp; Embedding-based Approach</h5>

		<p>Here, we extract from the target image a feature embedding of its identity and from the source image an embedding of its pose. The embeddings are then combined and decoded to produce an image with the target's identity and the source's pose. A common focus of works adopting this approach is how to disentangle identity from pose when computing the embeddings.</p>

		<p>The work by Zhang et al. explores using a semantic segmentation map of the face as pose embedding <a href="#fn_zhang_2019">[2019]</a>. X2Face uses the frontalized face as the embedding for identity and a formatless feature tensor as the embedding for pose. It removes identity information in the pose embedding through a training phase where the identity of the output image is enforced to be the same as that of the target image with the help of a face recognizer <a href="#fn_wiles_2018_2">[Wiles et al. 2018]</a>. Works by Burkov et al. <a href="#fn_burkov_2020">[2020]</a>and Zheng et al. <a href="#fn_zeng_2020">[2020]</a> require a target video as input. An encoder is biased to extract identity information by having all embeddings for the target video frames combined before being fed to the decoder. Another encoder is biased to extract pose by having their embeddings used to reconstruct each individual target frame. Huang et al. propose to extract pose embedding from detected landmarks rather than directly from source frames, and identity information is removed from pose embedding through adversarial training <a href="#fn_huang_2020">[2020]</a>. Usman et al. study the case where embeddings are generated from images in different domains, allowing, for example, a human face to be driven by 3D renderings <a href="#fn_usman_2019">[2019]</a>. Shalev and Wolf present an interesting method where pose is encoded with alpha masks and identity information is removed from it by thresholding on intensity values <a href="#fn_shalev_2020">[2020]</a>. </p>

		<div class="footnotes">				
			<ul>
				<ul>
					<li class="footnote" id="fn_zhang_2019">
						Yunxuan Zhang, Siwei Zhang, Yue He, Cheng Li, Chen Change Loy, and Ziwei Liu.
						<b>One-shot Face Reenactment.</b>
						BMVC 2019. 
						<a href="https://arxiv.org/abs/1908.03251">[arXiv]</a>
					</li>
				</ul>
				<li class="footnote" id="fn_wiles_2018_2">
	            	Olivia Wiles, A. Sophia Koepke, Andrew Zisserman.
	            	<b>X2Face: A network for controlling face generation by using images, audio, and pose codes.</b>
	            	ECCV 2018.
	            	<a href="https://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html">[Project]</a>
	            </li>
	            <li class="footnote" id="fn_burkov_2020">
	            	Egor Burkov, Igor Pasechnik, Artur Grigorev, and Victor Lempitsky.
	            	<b>Neural Head Reenactment with Latent Pose Descriptors</b>
	            	CVPR 2020.
	            	<a href="https://arxiv.org/abs/2004.12000v1">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_zeng_2020">
	            	Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, and Yong Liu.
	            	<b>Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose.</b>
	            	AAAI 2020.
	            	<a href="https://arxiv.org/abs/2003.12957">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_huang_2020">
	            	Po-Hsiang Huang, Fu-En Yang and and Yu-Chiang Frank Wang.
	            	<b>Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment.</b>
	            	CVPR 2020.
	            	<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_usman_2019">
	            	Ben Usman, Nick Dufour, Kate Saenko, and Chris Bregler.
	            	<b>PuppetGAN: Cross-Domain Image Manipulation by Demonstration.</b>
	            	ICCV 2019.
	            	<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Usman_PuppetGAN_Cross-Domain_Image_Manipulation_by_Demonstration_ICCV_2019_paper.pdf">[PDF]</a>
	            </li>
	            <li class="footnote" id="fn_shalev_2020">
	            	Yoav Shalev and Lior Wolf.
	            	<b>Image Animation with Perturbed Masks.</b>
	            	2020.
	            	<a href="https://arxiv.org/pdf/2011.06922.pdf">[PDF]</a>
	            </li>
			</ul>
		</div>

		<h5>2.1.2.4 &nbsp; Search-based Approach</h5>

		<p>This approach assumes that there is a database of images of the target subject. For each frame of the source video, we search for the most similar frame in the database and then combine the search results to create a video of the target subject mimicking the motion of the source. Kemelmacher-Shlizerman et al. build databases from photo collections, and they demonstrated motion transfer of celebrities with many Internet photographs <a href="#fn_kemelmacher-shlizerman_2010">[2010]</a>. Liu and Ostermann build databases from video recordings and use them to create talking head animations from speech <a href="#fn_liu_2011">[2011]</a>. Garrido et al. solve a related problem of swapping the source's face with that of the target's while maintaining the source motion <a href="#fn_garrido_2014">[2014]</a>. The database they use is a video of the target subject.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_kemelmacher-shlizerman_2010">
					 Ira Kemelmacher-Shlizerman, Aditya Sankar, Eli Shechtman, and Steve Seitz.
					 <b>Being John Malkovich.</b>
					 ECCV 2010.
					 <a href="https://grail.cs.washington.edu/malkovich/">[Project]</a>
				</li>
			</ul>
			<ul>
				<li class="footnote" id="fn_liu_2011">
					Kang Liu and Joern Ostermann.
					<b>Realistic facial expression synthesis for an image-based talking head.</b>
					IEEE International Conference on Multimedia and Expo 2011
					<a href="https://ieeexplore.ieee.org/document/6011835">[Paper]</a>
				</li>
			</ul>
			<ul>
				<li class="footnote" id="fn_garrido_2014">
					Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormahlen, Patrick Perez, and Christian Theobalt.
					<b>Automatic Face Reenactment.</b>
					CVPR 2014.
					<a href="https://gvv.mpi-inf.mpg.de/projects/FaceReenactment/files/FaceReenactment.pdf">[PDF]</a>
				</li>
			</ul>
		</div>

		<h3>2.2 &nbsp; My Previous Work</h3>

		<p>My <a href="https://pkhungurn.github.io/talking-head-anime/">2019 article</a> describes a neural network system that can animate the head of an existing anime character, given only an image of it. The pose of the character is specified by a six-dimensional pose vector whose components correspond to the movements depicted in Figure 2.1. Using the taxonomy discussed in the last subsection, the system solves the <i>parameter-based posing</i> problem with the <i>image-based approach</i>.</p>

		<p>
		<table align="center" cellpadding="5">
			<tr>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/right-eye.mp4" type="video/mp4">
					</video>
					<br>
					Right eye wink
				</td>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/left-eye.mp4" type="video/mp4">
					</video>
					<br>
					Left eye wink 
				</td>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/lips.mp4" type="video/mp4">
					</video><br>
					Mouth open/close
				</td>
			</tr>
			<tr>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/head-x.mp4" type="video/mp4">
					</video>
					<br>
					Rotate head around $x$-axis
				</td>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/head-y.mp4" type="video/mp4">
					</video>
					<br>
					Rotate head around $y$-axis
				</td>
				<td align="center">
					<video id="eyecatcher" autoplay muted playsinline loop width="200">
					    <source src="data/old_motion_samples/neck-z.mp4" type="video/mp4">
					</video>
					<br>
					Rotate neck around $z$-axis
				</td>
			</tr>
		</table>
		<b>Figure 2.1</b> Six types of movement my 2019 network can control. The character is <a href="https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA/videos">Kizuna AI</a> (© Kizuna AI).
		</p>

		<p>The system poses the given character in two steps, each carried out by a separate subnetwork. The <b>face morpher</b> modifies facial expression by closing the eyes and the mouth, taking care of the first 3 types of movement. The <b>face rotator</b> rotates the face, taking care of the other 3.</p>

		<p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/overview_two_step_process.png"><img src="data/overview_two_step_process.png" width="600"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 2.2</b> 
	                    An overview of how the 2019 system poses a character's head.
	                </td>
	            </tr>
	        </table>
	    </p>

	    <p>As mentioned earlier, the network builds heavily on two previous works. The face morpher is Pumarola et al.'s attention-based generator <a href="#fn_pumarola_2018_3">[2018]</a> unmodified. The face rotator was designed by combining Pumarola et al's generator with Zhou et al.'s appearance flow algorithm <a href="#fn_zhou_2016_3">[2016]</a>. The main insight was that one can generate high-quality rotated faces by combining these two algorithms: Zhou et al.'s would reuse the input pixels to generate sharp faces, and Pumarola et al's would hallucinate sensible pixels in disoccluded areas where Zhou et al.'s would have difficulty finding the right pixels to copy from.</p>

	    <div class="footnotes">
			<ul>
				<li class="footnote" id="fn_pumarola_2018_3">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
				<li class="footnote" id="fn_zhou_2016_3">
					Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros.
					<b>View Synthesis by Appearance Flow.</b> 
					ECCV 2016. 
					<a href="https://arxiv.org/abs/1605.03557">[arXiv]</a>
				</li>
			</ul>
		</div>

		<p>I also introduced a way to generate high quality data by using downloadable 3D models created for the <a href="https://sites.google.com/view/vpvp/">MikuMikuDance</a> (MMD) software. I collected and annotated more than 8,000 models and used them to generate a large (500,000 examples) training set. While the system was trained on 3D renderings, I showed that it generalized well to hand-drawn characters.</p>

		<p>The system, however, cannot yet be considered practical for becoming a VTuber. It demands a lot of computational power, requiring a powerful GPU to generate real-time animations. The generated images are blurry in disoccluded areas. Most importantly, it can only close the eyes and mouth, robbing the character the ability to make most facial expressions.</p>

		<a name="overview"></a>
		<h2>3 &nbsp; Overview</h2>

		<p>My goal in this article is to solve the previous system's poverty of facial expressions. To this end, I propose a new architecture for the face morpher network, the overview of which is depicted in Figure 3.1.</p>

		<p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/face_morpher_two_steps.png"><img src="data/face_morpher_two_steps.png" width="600"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="left">
	                    <b>Figure 3.1</b> 
	                    An overview of the new face morpher architecture. It morphs the face in two steps. The first morphs the eyebrow, and the second morphs the eyes and the mouth. The character is <a href="https://www.youtube.com/channel/UCp6993wxpyDPHUpavwDFqgg">Tokino Sora</a> (© Tokino Sora Ch.).
	                </td>
	            </tr>
	        </table>
	    </p>

	    <p>The new face morpher has two subnetworks: the <b>eyebrow morpher</b> and the <b>eye & mouth morpher</b>. Each network only deforms the organ(s) in its name, and the eyebrow morpher is applied to the input image first. The pose vector is now assumed to contain two parts: one describes the eyebrows, and the other the eyes and the mouth. (More details in Section 4.) The parts are fed to their respective networks. During research, I discovered that it was important to process the eyebrows separately from the rest of the organs, and I will elaborate on this point in Section 7.3. However, the order of the networks is arbitrary. An alternative design that applies the eye & morpher first would work just as well.</p>

	    <p>The eyebrow morpher operates on the middle $128 \times 128$ subimage of the input image, which, according to the <a href="https://pkhungurn.github.io/talking-head-anime/index.html#problem-spec">input specification</a> of my 2019 paper, contains the entire face of the character. It first segments out the eyebrows with a dedicated subnetwork called the <b>eyebrow segmenter</b>. It then uses another subnetwork called the <b>eyebrow warper</b> to deform eyebrow and then composite the result back to the original image. I will discuss the eyebrow morpher's architecture in more details in Section 6.2.</p>

	    <p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/eyebrow_morpher_overview.png"><img src="data/eyebrow_morpher_overview.png" width="600"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 3.2</b> 
	                    An overview of the architecture of the eyebrow morpher.
	                </td>
	            </tr>
	        </table>
	    </p>

	    <p>The eye & mouth morpher, on the other hand, operates on the middle $192 \times 192$ subimage of the output of the last step. It consists of three main parts. The first is an encoder-decoder network that produces a feature tensor that will be used to transform the input. The second uses it to morph the mouth and the irises. The third uses the output of the previous part and the feature tensor to morph the eyelids, completing the face morphing process. While the output of the second step is not used by any subsequent parts of the whole system, it is used in the training process of the morpher itself. I will discuss what the image features and the network parts are in Section 6.3.</p>

	    <p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/eye_and_mouth_morpher_overview.png"><img src="data/eye_and_mouth_morpher_overview.png" width="600"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 3.3</b> 
	                    An overview of the architecture of the eye & mouth morpher.
	                </td>
	            </tr>
	        </table>
	    </p>

	    <p>Similar to what was done in the 2019 article, I will also generate training data from MMD models. However, they now require more preparation as the poses have become more complex. I will discuss data preparation in Section 5.</p>

	    <a name="problem-spec"></a>
	    <h2>4 &nbsp Problem Specification</h2>

		<p>Before discussing the network architectures and the datasets in details, let us first precisely specify the problem we are solving. The system takes two input: an image of an anime character and a pose vector.</p>

		<p><b>Input image.</b> The character image must be of size $256 \times 256$ and is in RGBA format. Additionally, for any pixel with $A = 0$, it must be the case that $R = G = B = 0$ too. We assume that the character is in a "rest pose," and the head is contained to be roughly in the middle of the image. See the detailed specification in Figure 4.1.</p>

		 <p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/input_image_spec.png"><img src="data/input_image_spec.png" width="600"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="left">
	                    <b>Figure 4.1</b> The input image must depict a character looking straight ahead with eyes wide open. The mouth can be close or wide open. If the mouth is closed, the head should be contained in the middle $128 \times 128$ square as depicted in the left image. If the mouth is open, most of the head should still be contained in the square, but the chin can be below its bottom edge. In all cases, the whole head should be contained in the middle $192 \times 192$ square as depicted in the right image. It is important to for these constraints to hold because the eyebrow morpher operates on the middle $128 \times 128$ square and the eye & mouth morpher operates on the $192 \times 192$.
	                </td>
	            </tr>
	        </table>
	    </p>

	    <p><b>Pose vector.</b> The pose vector is a collection of 42 parameters. Three correspond to rotation of the head and neck joints around the three axes. They are the same as those in the 2019 article.</p>

	    <p>
	    	<table align="center" cellpadding="5">		    		
	    		<tr>
	    			<td align="center">
	    				<video id="eyecatcher" autoplay muted playsinline loop width="128">
					    	<source src="data/motion_samples/souya_ichika/head_x/video.mp4" type="video/mp4">
						</video>
						<br>
						<tt>head_x</tt>
	    			</td>
	    			<td align="center">
	    				<video id="eyecatcher" autoplay muted playsinline loop width="128">
					    	<source src="data/motion_samples/souya_ichika/head_y/video.mp4" type="video/mp4">
						</video>
						<br>
						<tt>head_y</tt>						
	    			</td>
	    			<td align="center">
	    				<video id="eyecatcher" autoplay muted playsinline loop width="128">
					    	<source src="data/motion_samples/souya_ichika/neck_z/video.mp4" type="video/mp4">
						</video>
						<br>
						<tt>neck_z</tt>						
	    			</td>
	    		</tr>
	    		<tr>
	                <td colspan="3" align="left">
	                    <b>Figure 4.2</b> Pose parameters for face rotation. The character is <a href="https://www.youtube.com/channel/UC2kyQhzGOB-JPgcQX9OMgEw">Souya Ichika</a> (&copy; 774 Inc.).
	                </td>
	            </tr>		    		
	    	</table>
	    </p>

	    <a name="face-params"></a>
	    <p>Two parameters correspond to the rotation of the irises <a href="#fn_iris_rotation">[footnote]</a>. One is for rotation around the $x-$axis, and the other for the $y-$. They take values in the $[-1,1]$ range. The value $0$ corresponds to being at rest (i.e., looking straight ahead), and the values $\pm 1$ correspond to the left/right or top/down extremes.</p>

	    <div class="footnotes">
	    	<ul>
	    		<li class="footnote" id="fn_iris_rotation">
	    			It is not possible for a human to rotate their irises. Rather, we rotate our eyeballs. However, I created training data from MMD models, and they have meshes for the irises separated from the rest of the body. The gaze direction can be manipulated by rotating the iris meshes, so I call the associated parameters the "iris rotation" parameters.
	    		</li>
	    	</ul>
	    </div>

	    <p>
	    	<table align="center" cellpadding="5">		    		
	    		<tr>
	    			<td align="center">
	    				<video id="eyecatcher" autoplay muted playsinline loop>
					    	<source src="data/motion_samples/souya_ichika/iris_rotation_x/video.mp4" type="video/mp4">
						</video>
						<br>
						<tt>iris_rotation_x</tt>
	    			</td>
	    			<td align="center">
	    				<video id="eyecatcher" autoplay muted playsinline loop>
					    	<source src="data/motion_samples/souya_ichika/iris_rotation_y/video.mp4" type="video/mp4">
						</video>
						<br>
						<tt>iris_rotation_y</tt>
	    			</td>		    			
	    		</tr>
	    		<tr>
	                <td colspan="3" align="center">
	                    <b>Figure 4.3</b> Pose parameters for iris rotation.
	                </td>
	            </tr>		    		
	    	</table>
	    </p>

	    <p>The remaining 37 parameters come from 37 common blendshapes in the MMD models I used to create the training data. They take values in the range $[0,1]$, and each is the weight of its corresponding blendshape. That is, the value $0$ means that the blendshape is not used and has no effect on the model's shape. On the other hand, $1$ means the blendshape is fully activated. The parameters can be organized into 22 "semantics" (i.e., what they are supposed to do to the face; for examples, "say 'ah'" or "make 'angry' eyebrows"), and these are listed in the tables below.</p>

	    <p>
	    	<table class="table table-striped">
	    		<thead class="table-dark">
	    			<tr>
	    				<th scope="col">Semantics</th>
	    				<th scope="col"># Params</th>
		    			<th scope="col">Common blendshape names (Japanese)</th>
		    			<th scope="col">Video</th>
	    			</tr>		    			
	    		</thead>
	    		<tbody>
	    			<tr>
	    				<td><tt>eyebrow_troubled</tt></td>
	    				<td>2</td>
	    				<td>困る、困り</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_troubled/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eyebrow_angry</tt></td>
	    				<td>2</td>
	    				<td>怒る、怒り</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_angry/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eyebrow_serious</tt></td>
	    				<td>2</td>
	    				<td>真面目</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_serious/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eyebrow_happy</tt></td>
	    				<td>2</td>
	    				<td>にこり、にこ、にこっ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_happy/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eyebrow_lowered</tt></td>
	    				<td>2</td>
	    				<td>下、眉下</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_lowered/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eyebrow_raised</tt></td>
	    				<td>2</td>
	    				<td>上、眉上</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eyebrow_raised/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>		    			
	    		</tbody>		    		
	    	</table>
	    	<b>Table 4.4</b> 6 semantics that change the eyebrows. Each has 2 parameters &mdash; one for the left eyebrow and one for the right &mdash; resulting in 12 parameters in total.
	    </p>

	    <p>
	    	<table class="table table-striped">
	    		<thead class="table-dark">
	    			<tr>
	    				<th scope="col">Semantics</th>
	    				<th scope="col"># Params</th>
		    			<th scope="col">Common blendshape names (Japanese)</th>
		    			<th scope="col">Video</th>
	    			</tr>		    			
	    		</thead>
	    		<tbody>
	    			<tr>
	    				<td><tt>eye_wink</tt></td>
	    				<td>2</td>
	    				<td>まばたき、ウィンク</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_wink/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eye_happy_wink</tt></td>
	    				<td>2</td>
	    				<td>笑い</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_happy_wink/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eye_relaxed</tt></td>
	    				<td>2</td>
	    				<td>なごみ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_relaxed/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eye_unimpressed</tt></td>
	    				<td>2</td>
	    				<td>ジト目</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_unimpressed/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eye_raised_lower_eyelid</tt></td>
	    				<td>2</td>
	    				<td>下瞼上げ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_raised_lower_eyelid/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>eye_surprised</tt></td>
	    				<td>2</td>
	    				<td>びっくり</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/eye_surprised/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>		    	
	    			<tr>
	    				<td><tt>iris_small</tt></td>
	    				<td>2</td>
	    				<td>瞳小</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/iris_small/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>		    			
	    		</tbody>		    		
	    	</table>
	    	<b>Table 4.5</b> 7 semantics for controlling the eyes and the irises. Again, each semantic has 2 parameters for the left side and the right, resulting in 14 parameters in total.
	    </p>

	    <p>
	    	<table class="table table-striped">
	    		<thead class="table-dark">
	    			<tr>
	    				<th scope="col">Names</th>
	    				<th># Params</th>
		    			<th scope="col">Common blendshape names (Japanese)</th>
		    			<th scope="col">Video</th>
	    			</tr>		    			
	    		</thead>
	    		<tbody>
	    			<tr>
	    				<td><tt>mouth_aaa</tt></td>
	    				<td>1</td>
	    				<td>あ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_aaa/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_iii</tt></td>
	    				<td>1</td>
	    				<td>い</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_iii/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_uuu</tt></td>
	    				<td>1</td>
	    				<td>う</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_uuu/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_eee</tt></td>
	    				<td>1</td>
	    				<td>え</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_eee/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_ooo</tt></td>
	    				<td>1</td>
	    				<td>お</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_ooo/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_delta</tt></td>
	    				<td>1</td>
	    				<td>Δ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_delta/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_smirk</tt></td>
	    				<td>1</td>
	    				<td>はんっ！</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_smirk/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>		    	
	    			<tr>
	    				<td><tt>mouth_raised_corner</tt></td>
	    				<td>2</td>
	    				<td>口角上げ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_raised_corner/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>
	    			<tr>
	    				<td><tt>mouth_lowered_corner</tt></td>
	    				<td>2</td>
	    				<td>口角下げ</td>
	    				<td>
	    					<video id="eyecatcher" autoplay muted playsinline loop>
						    	<source src="data/motion_samples/souya_ichika/mouth_lowered_corner/video.mp4" type="video/mp4">
							</video>
	    				</td>
	    			</tr>		    			
	    		</tbody>		    		
	    	</table>
	    	<b>Table 4.6</b> 7 semantics (11 parameters) for controlling the mouth. Each semantic involving the mouth corners has two parameters: one for the left corner and one for the right. 
	    </p>

	    <p>In order for the deformed shape of the face to be well defined, the weights of the blendshapes that modify the same facial feature must not add up to be more than $1$ <a href="#fn_lewis_2014">[Lewis et al. 2014]</a>. More specifically, suppose $k$ blendshapes modify the same feature, say the left eyebrow. Let us denote the values of their weights be denoted by $\beta_1$, $\beta_2$, $\dotsc$, $\beta_k$. Then, it must hold that 
	    \begin{equation}
	    \beta_1 + \beta_2 + \dotsb + \beta_k \leq 1. \tag{1}	    
	    \end{equation}
	    In my case, there is such a constraint for each of the two eyebrows, the two eyes, the two irises, and the mouth.
	    </p>

	   	<div class="footnotes">
	   		<ul>
	   			<li class="footnote" id="fn_lewis_2014">
	   				J.P. Lewis, Ken Anjyo, Taehyun Rhee, Mengjie Zhang, Fred Pighin, and Zhigang Deng.
	   				<b>Practice and Theory of Blendshape Facial Models.</b>
	   				Eurographics 2014 State of the Art Reports.
	   				<a href="https://diglib.eg.org/xmlui/bitstream/handle/10.2312/egst.20141042.199-218/199-218.pdf?sequence=1&isAllowed=y">[PDF]</a>
	   			</li>
	   		</ul>
	   	</div>

	   	<p>Lastly, recall that the system, taking the character image and the 42-dimensional pose vector as input, must produce a new image of the same character with the pose specified by the pose vector.</p>

		<a name="data"></a>
		<h2>5 &nbsp; Data</h2>

		<p>To generate data to train and evaluate the networks in Section 3, I reuse the collection of approximately 8,000 MMD models I collected for my last work. (See more details <a href="https://pkhungurn.github.io/talking-head-anime/index.html#dataset">here</a>.) The current work, through, requires more annotations and a different way to generate training examples because the pose vector has become more complicated.</p>

		<h3>5.1 &nbsp; Data Annotation</h3>

		<p>To generate a training dataset containing all variations specified by the pose vector, I need the following pieces of information about each MMD model.
		<ol> 
			<li>The location of the head so that the I can render it at the center of the image.</li>
			<li>The angles to rotate the irises by in order to define the semantics of the <tt>iris_rotation_x</tt> and <tt>iris_rotation_y</tt> parameters.</li>
			<li>The names of the blendshapes corresponding to the 37 parameters in the last section.</li>
			<li>The vertices that constitute the eyebrows so that I can generate ground truth renderings of the segmented and warped eyebrows to train the eyebrow morphers.</li>
		</ol>
		I already obtained the first piece through manual annotation for the 2019 project. I will now discuss how I acquire the other three.
		</p>

		<h4>5.1.1 &nbsp; Iris Rotation Annotations</h4>

		<p>While most of the 8,000 models allow their irises to be rotated, specifying the same rotation angle can yield very different results in different models. There are several reasons for this: the centers of rotation are different, the iris sizes are different, and some models even damp or amplify the rotation angles.</p>

		<p>To get more consistent semantics for the two iris rotation parameters, I sought to specify the irises' movement ranges visually. To do so, I first need to know the 2D bounding boxes of the eyes, and I created a tool to manually specify them, as shown in the video below.</p>

		<p align="center"> 
			<iframe width="560" height="315" src="https://www.youtube.com/embed/g5bEUtfbJ0o" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>

		<p>To define the semantics of the <tt>iris_rotation_x</tt> parameter, which represents vertical movement, I divide the eye bounding box vertically into 4 horizontal strips. The meanings of the values $-1$ and $1$ are as in Figure 5.1.</p>

		<p>
			<table align="center">
				<tr>
					<td align="center"><img src="data/iris_rotation/normal_eye.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_yyy_negative.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_yyy_positive.png" width="180"></td>
				</tr>
				<tr>
					<td align="center"><img src="data/iris_rotation/normal_eye_horizontal_strips.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_yyy_negative_strips.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_yyy_positive_strips.png" width="180"></td>
				</tr>
				<tr>
					<td align="center">(a)</td>
					<td align="center">(b) $\mathrm{iris\_rotation\_x} = -1$</td>
					<td align="center">(c) $\mathrm{iris\_rotation\_x} = 1$</td>
				</tr>
				<tr>
					<td colspan="3"><b>Figure 5.1</b> (a) An eye whose bounding box is divided into 4 horizontal strips. (b) The value $-1$ of the <tt>iris_rotation_x</tt> parameter corresponds to the rotation angle such that the topmost part of the iris touches the bottommost part of the first strip from the top. (c) The value $1$ corresponds to the rotation angle such that the bottommost part of the iris touches the topmost part of the third strip from the top (which is the first strip from the bottom).</td>
				</tr>
			</table>
		</p>

		<p>For the <tt>iris_rotation_y</tt> parameter, which represents horizontal movement, I divide the eye bounding box horizontally into 8 vertical strips. The meanings of the values $-1$ and $1$ are as in Figure 5.2.</p>

		<p>
			<table align="center">
				<tr>
					<td align="center"><img src="data/iris_rotation/normal_eye.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_xxx_negative.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_xxx_positive.png" width="180"></td>
				</tr>
				<tr>
					<td align="center"><img src="data/iris_rotation/normal_eye_vertical_strips.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_xxx_negative_strips.png" width="180"></td>
					<td align="center"><img src="data/iris_rotation/iris_rotation_xxx_positive_strips.png" width="180"></td>
				</tr>
				<tr>
					<td align="center">(a)</td>
					<td align="center">(b) $\mathrm{iris\_rotation\_y} = -1$</td>
					<td align="center">(c) $\mathrm{iris\_rotation\_y} = 1$</td>
				</tr>
				<tr>
					<td colspan="3"><b>Figure 5.2</b> (a) An eye whose bounding box is divided into 8 vertical strips. (b) The value $-1$ of the <tt>iris_rotation_y</tt> parameter corresponds to the rotation angle such that the leftmost part of the iris touches the rightmost part of the third strip from the left. (c) The value $1$ corresponds to the rotation angle such that the rightmost part of the iris touches the leftmost part of the sixth strip from the left (which is the third strip from the right).</td>
				</tr>
			</table>
		</p>

		<p>Again, I created another tool to specify the angles that correspond to the $\pm 1$ values.</p>

		<p align="center"> 
			<iframe width="560" height="315" src="https://www.youtube.com/embed/goXyU6JwKSY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>

		<h4>5.1.2 &nbsp; Blendshape Annotations</h4>

		<p>To be able to generate images of MMD models making facial expressions according to the pose parameters in Section 4, I need to know the names of the blendshapes that correspond to each of the 37 parameters. Unfortunately, the raw model data were dirty. Different modelers name blendshapes differently, and most models do not have the full set of blendshapes. Moreover, for facial features that have two sides (e.g., the eyes), most models do not allow the sides to be controlled independently. Some even only have a blendshape for one side and lack the other. I thus had no choice but to find out the blendshape names manually. I created another tool to make these annotations, and it is shown in the video below.</p>

		<p align="center"> 
			<iframe width="560" height="315" src="https://www.youtube.com/embed/Xhzcd8ZBlKs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>

		<p>The table below shows the result of my annotation work.</p>

		<p>
	    	<table class="table table-striped">
    			<tr class="table-dark">
    				<td scope="col"><b>Semantics</b></td>
	    			<td align="right" scope="col"><b>Both/No Sides</b></td>
	    			<td align="right" scope="col"><b>Left Side Only</b></td>
	    			<td align="right" scope="col"><b>Right Side Only</b></td>
    			</tr>		    			
    			<tr>
    				<td><tt>eyebrow_troubled</tt></td>
    				<td align="right">7,311</td>
    				<td align="right">2,332</td>
    				<td align="right">2,332</td>
    			</tr>
    			<tr>
    				<td><tt>eyebrow_angry</tt></td>
    				<td align="right">7,077</td>
    				<td align="right">2,249</td>
    				<td align="right">2,249</td>
    			</tr>
    			<tr>
    				<td><tt>eyebrow_lowered</tt></td>
    				<td align="right">6,757</td>
    				<td align="right">2,094</td>
    				<td align="right">2,094</td>
    			</tr>
    			<tr>
    				<td><tt>eyebrow_raised</tt></td>
    				<td align="right">6,667</td>
    				<td align="right">2,216</td>
    				<td align="right">2,216</td>
    			</tr>
    			<tr>
    				<td><tt>eyebrow_happy</tt></td>
    				<td align="right">5,936</td>
    				<td align="right">1,661</td>
    				<td align="right">1,661</td>
    			</tr>
    			<tr>
    				<td><tt>eyebrow_serious</tt></td>
    				<td align="right">5,847</td>
    				<td align="right">1,526</td>
    				<td align="right">1,526</td>
    			</tr>
    			<tr>
    				<td><tt>eye_wink</tt></td>
    				<td align="right">7,674</td>
    				<td align="right">6,597</td>
    				<td align="right">6,662</td>
    			</tr>
    			<tr>
    				<td><tt>eye_happy_wink</tt></td>
    				<td align="right">7,380</td>
    				<td align="right">6,851</td>
    				<td align="right">6,958</td>
    			</tr>
    			<tr>
    				<td><tt>eye_surprised</tt></td>
    				<td align="right">6,329</td>
    				<td align="right">1,025</td>
    				<td align="right">1,025</td>
    			</tr>
    			<tr>
    				<td><tt>eye_relaxed</tt></td>
    				<td align="right">4,727</td>
    				<td align="right">291</td>
    				<td align="right">291</td>
    			</tr>
    			<tr>
    				<td><tt>eye_unimpressed</tt></td>
    				<td align="right">6,165</td>
    				<td align="right">1,299</td>
    				<td align="right">1,299</td>
    			</tr>
    			<tr>
    				<td><tt>eye_raised_lower_eyelid</tt></td>
    				<td align="right">1,646</td>
    				<td align="right">770</td>
    				<td align="right">770</td>
    			</tr>
    			<tr>
    				<td><tt>iris_small</tt></td>
    				<td align="right">6,452</td>
    				<td align="right">731</td>
    				<td align="right">731</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_aaa</tt></td>
    				<td align="right">7,584</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_iii</tt></td>
    				<td align="right">7,691</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_uuu</tt></td>
    				<td align="right">7,648</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_eee</tt></td>
    				<td align="right">6,992</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_ooo</tt></td>
    				<td align="right">7,575</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_delta</tt></td>
    				<td align="right">3,310</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_smirk</tt></td>
    				<td align="right">775</td>
    				<td align="right">&mdash;</td>
    				<td align="right">&mdash;</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_raised_corner</tt></td>
    				<td align="right">4,770</td>
    				<td align="right">856</td>
    				<td align="right">856</td>
    			</tr>
    			<tr>
    				<td><tt>mouth_lowered_corner</tt></td>
    				<td align="right">4,826</td>
    				<td align="right">485</td>
    				<td align="right">485</td>
    			</tr>
	    	</table>
	    	<b>Table 5.3</b> The number of models I marked as having blendshapes that conform to the 22 semantics in Section 3. For any facial feature with bilateral symmetry, there are three possible blendshapes: those that control both sides of the feature simultaneously, those that control only the left side, and those that control only the right side.
	    </p>

		<h4>5.1.3 &nbsp; Eyebrow Annotations</h4>

		<p>To train the eyebrow morpher, I need to render the eyebrows apart from the rest of the model. To do so, I must know the vertices that constitute the eyebrows. In general, the list of vertices can be found automatically by gathering all vertices that are modified by eyebrow-morphing blendshapes. However, raw data were again dirty, and I had to manually exclude models whose eyebrow-morphing blendshapes modify unrelated vertices. How I performed this type of annotation is best described in Section 5.2.2.</p>

		<h3>5.2 &nbsp; Dataset Generation</h3>

		<h4>5.2.1 &nbsp; Composition of a Training Example</h4>

		<p>A training example contains (1) an image of a character in a rest pose, (2) a pose vector, and (3) ground truths of the images that are outputted by the subnetworks in Section 3. There are 7 ground truth images, and they depict:
		<ol style="list-style-type: lower-alpha;"> 
			<li>the character in a rest pose with the eyebrows removed;</li>
			<li>the eyebrows at rest;</li>
			<li>the eyebrows after being morphed according to the pose vector;</li>
			<li>the character with the morphed eyebrows;</li>
			<li>the character with the morphed eyebrows, irises, and mouth;</li>
			<li>the character with all the facial features morphed; and</li>
			<li>the character with all facial features morphed and its face rotated.</li>
		</ol>
		To ease further discussion, we will refer to these images as $GT_a$, $GT_b$, and so on. The images are shown in the figure below.
		</p>

		<p> 
			<table align="center" cellpadding="5"> 
				<tr>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_eyebrow_0.png"> 
						</div>
						$GT_a$
					</td>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_eyebrow_1.png"> 
						</div>
						$GT_b$
					</td>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_eyebrow_2.png"> 
						</div>
						$GT_c$
					</td>
				</tr>
				<tr>	
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_1.png"> 
						</div>
						$GT_d$
					</td>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_3.png"> 
						</div>
						$GT_e$
					</td>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_4.png"> 
						</div>
						$GT_f$
					</td>
				</tr>
				<tr>
					<td align="center">
						<div style="overflow: hidden; width: 192px; height: 192px; border-width: 1px; border-style: solid;">
							<img style="margin-top: -32px; margin-left: -32px" src="data/training_examples/ushimaki_riko/00000033_5.png"> 
						</div>
						$GT_g$
					</td>
					<td align="center">
						&nbsp;
					</td>
					<td align="center">
						&nbsp;
					</td>
				</tr>
				<tr> 
					<td colspan="3" align="left"> 
						<b>Figure 5.4</b> Ground truth images in each training example. The character is <a href="https://www.youtube.com/channel/UCKUcnaLsG2DeQqza8zRXHiA">Ushimaki Riko</a> (© Appland, Inc.).
					</td>
				</tr>
			</table>
		</p>

		<p>The ground truth images serve the following purposes:
		<ul> 
			<li>$GT_a$ and $GT_b$ are used to train the eyebrow segmenter.</li> 
			<li>$GT_c$ and $GT_d$ are used to train the eyebrow warper.</li>
			<li>$GD_e$ and $GT_f$ are used to train the eye & mouth morpher.</li>
			<li>$GD_g$ is used for evaluation purposes. See Section 7.</li>
		</ul>
		</p>
		
		<p>The ground truth images are generated by deforming the models according to the relevant parameters in the pose vector. That is,
		<ul> 
			<li>$GT_c$ and $GT_d$ are generated using only the parameters that control the eyebrows (i.e., those in Table 4.4).</li>
			<li>$GT_e$ is generated using the eyebrow parameters, the mouth parameters (Table 4.6), the <tt>iris_small</tt> parameter, the iris rotation parameters in Figure 4.3, and, lastly, the <tt>eye_surprised</tt> parameters.</li>
			<li>$GT_f$ is generated using all of the above parameters plus the remaining eye parameters in Table 4.5.</li>
			<li>$GT_g$ is generated using all parameters of the pose vector.</li>
		</ul>
		The reader may notice that I include <tt>eye_surprised</tt> in $GT_e$ before $GT_f$ while the latter is especially supposed to depict changes to the eyes. The reason is that <tt>eye_surprised</tt> actually <i>opens</i> the eyes instead of closing them like other eye-related semantics. Since opening the eyes involves hallucinating unseen parts of the irises, I also include <tt>eye_surprised</tt> in $GT_e$, which is supposed to depict changes to the irises in addition to the mouth.
		</p>

		<h4>5.2.2 &nbsp; Augmented Eyebrow Renderings</h4>

		<p>During research, I encountered difficulties training networks to manipulate eyebrows in drawings. One of the causes is that drawn eyebrows are not similar to those in 3D renderings. Based on personal experience, I have identified two different ways in which eyebrows are composited:
		<ol> 
			<li>Eyebrows are shown underneath hair bangs.</li> 
			<li>Eyebrows are shown over hair bangs as depicted earlier in Figure 1.1.</li>
		</ol>
		Moreover, I also observed two ways in which they are drawn and colored.		
		</p>

		<p> 
			<table align="center" cellpadding="5"> 
				<tr> 
					<td align="center"> 
						<img src="data/out_of_order_eyebrows/kiryuu_koko.png" border="1"><br> 						
						<a href="https://www.youtube.com/channel/UCS9uQI-jC3DE0L4IpXyvr6w">Kiryu Coco</a><br>
						&copy; COVER Corp.
					</td>
					<td>a. Eyebrows are filled with a color darker than the hair.</td>
				</tr>
				<tr>
					<td align="center">
						<img src="data/out_of_order_eyebrows/usada_pekora.png" border="1"><br>						
						<a href="https://www.youtube.com/channel/UC1DCedRgGHBdm81E1llLhOQ">Usada Pekora</a><br>
						&copy; COVER Corp.
					</td>
					<td>b. Eyebrows are filled with the hair's color but have dark outlines.</td>
				</tr>				
			</table>
		</p>

		<p>We can now see why 3D-rendered eyebrows are not similar to drawn eyebrows. They always appear underneath hair bangs. Moreover, they mostly have the same color as the hair and do not have outlines (called "<a href="https://en.wikipedia.org/wiki/Silhouette_edge">silhouette edges</a>" in 3DCG terminology). To generate more "realistic" eyebrows, I introduced the following 4 "modes" for rendering them.</p>

		<p> 
			<table align="center" cellpadding="5"> 
				<tr> 
					<td>
						<div style="overflow: hidden; width: 256px; height: 128px; border-width: 1px; border-style: solid;"> 
							<img src="data/eyebrow_render_modes/DO_NOTHING.png" style="margin-top: -128px; margin-left: -64px" width="384"> 
						</div>
					</td>
					<td>The <b><i>normal</i></b> mode renders the model normally. The eyebrows have the color that comes with the model and are rendered according to the z-order specified therein.</td>
				</tr>
				<tr> 
					<td>
						<div style="overflow: hidden; width: 256px; height: 128px; border-width: 1px; border-style: solid;"> 
							<img src="data/eyebrow_render_modes/MERGE_MASKED_IGNORE_DEPTH.png" style="margin-top: -128px; margin-left: -64px" width="384"> 
						</div>
					</td>
					<td>The <b><i>eyebrow-on-top</i></b> mode uses the model's eyebrow color but renders the eyebrows on top of other parts of the mesh.</td>
				</tr>
				<tr> 
					<td>
						<div style="overflow: hidden; width: 256px; height: 128px; border-width: 1px; border-style: solid;"> 
							<img src="data/eyebrow_render_modes/MERGE_BLACK_MASKED_IGNORE_DEPTH.png" style="margin-top: -128px; margin-left: -64px" width="384"> 
						</div> 
					</td>
					<td>The <b><i>black</i></b> mode renders the eyebrows black and on top of other parts of the mesh.</td>
				</tr>
				<tr> 
					<td> 
						<div style="overflow: hidden; width: 256px; height: 128px; border-width: 1px; border-style: solid;"> 
							<img src="data/eyebrow_render_modes/MERGE_MASKED_WITH_BLACK_SILHOUETTE_IGNORE_DEPTH.png" style="margin-top: -128px; margin-left: -64px" width="384"> 
						</div> 
					</td>
					<td>The <b><i>silhouette</i></b> mode renders the eyebrows on top of other parts of the mesh and adds black silhouettes around them.</td>
				</tr>
			</table>
		</p>

		<p>However, not all models I gathered supported rendering the above images. I mentioned earlier in Section 5.1.3 that I identified eyebrow vertices by unioning all the vertices that are modified by eyebrow-related blendshapes <a href="#fn_eyebrow_vertex_identification">[footnote]</a>. Yet, for some models, these blendshapes do modify vertices of other facial features, so painting them black or putting silhouettes around them would destroy the characters' faces. As a result, I must exclude such a model from rendering the last three modes. Moreover, some modes would not be useful in some situations. For examples:
		<ul> 
			<li>The eyebrow-on-top mode would easily be confused with the normal mode if the eyebrows and the hair have similar colors.</li>
			<li>The black mode and the silhouette mode would be wasteful if the character's hair is already black.</li>
		</ul>
		To avoid confusing and wasteful training examples, I decided to throw away the eyebrow-on-top mode and the black mode if the rendered eyebrow color is similar to the surrounding hair color.</p>

		<div class="footnotes"> 
			<ul> 
				<li class="footnote" id="fn_eyebrow_vertex_identification">Note that there are other methods of identifying eyebrow vertices, but I thought that they would be less efficient and/or hard to implement.</li>
			</ul>
		</div>

		<p>In the video below, I show a tool that allows me to mark the rendering modes as usable or unusable. This tool is how I created the annotations I mentioned in Section 5.1.3.</p>

		<p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/JgCEyZiOMU8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>

		<h4>5.2.3 &nbsp; Sampling</h4>

		<p>To generate a training example, I need to sample the following pieces of data:
		<ul> 
			<li>the model to render,</li>
			<li>the pose vector, </li>
			<li>the mode in which to render the eyebrows,</li>
			<li>the parameters of the camera, and</li>
			<li>The lighting in the scene.</li>
		</ul>
		Let us discuss them in order of increasing sampling complexity.
		</p>

		<p><b>Camera parameters.</b> There is no sampling. The camera must perform an <a href="https://en.wikipedia.org/wiki/Orthographic_projection">orthographic projection</a> so that it is looking straight at the model with the head contained in the middle $128 \times 128$ square of the rendered image. Hence, the camera parameters are completely determined by the bounding box of the model's head.</p>

		<p><b>Model.</b> I decided up front how many examples to generate, and I distributed them equally among the models.</p>

		<p><b>Eyebrow rendering mode.</b> From the annotations discussed in the last section, each model should have a list of modes that are marked usable. I sample the usable modes uniformly when generating each example.</p>

		<p><b>Lighting.</b> I use the same lighting settings as in <a href="https://pkhungurn.github.io/talking-head-anime/index.html#rendering">my previous work.</a> All scenes contain two lights: an ambient one and a directional one. Both are white, and the directional light points in the negative $z$ direction. The lights' intensities are sampled a bit differently this time. The ambient light's intensity is sampled uniformly from the range $[0.5, 0.75]$, and the directional light's radiance is sampled uniformly from the range $[0.5, 1.25]$.</p>

		<p><b>Pose vectors.</b> While my system only consumes one pose vector, I need two pose vectors to generate a training example. The first is the pose vector for the input image, and the other for the six ground truth images.</p>

		<p>For the input image, the character needs to be in a rest pose, which means that all components of the pose vector is zero. There is an exception to this rule, however: the <tt>mouth_aaa</tt> parameter can be either $0$ or $1$ to allow the mouth to be fully closed or fully open as previously discussed in Figure 4.1. I sample the rest pose vector in such a way that the probably of <tt>mouth_aaa</tt> being $1$ is $75\%$.</p>

		<p>To sample a pose vector for the ground truth images, I divide the pose parameters into the following 6 groups:</p>		

		<ol>
			<li>the <tt>iris_rotation_x</tt> parameter;</li>
			<li>the <tt>iris_rotation_y</tt> parameter;</li> 
			<li>the two <tt>iris_small</tt> parameters;</li>
			<li>the eyebrow parameters in Table 4.4;</li>
			<li>the eye parameters in Table 4.5, excluding <tt>iris_small</tt>; and</li>
			<li>the mouth parameters in Table 4.6.</li>			
		</ol>

		<p>The groups can be sampled independently of one another because what each group controls does not interfere with those of other groups. I will now discuss how to sample values for each group.		
		</p>

		<p>The iris rotation parameters (Group 1 and 2) are sampled independently and uniformly from the range $[-1,1]$.</p>

		<p>For Group 3, I set the members' values to $0$ with probability $25\%$ in order to allow the model to learn how to rotate the full-sized irises well. I do so because I anticipate that users of my system would rarely reduce the iris sizes. Doing so makes the character look angry or deranged, and it is rare for a character to express these temperaments all the time. For the rest $75\%$, I sample the values using the following case-by-case process:
		<ul> 
			<li>If the model has one blendshape for the left iris and one for the right, I sample the associated parameter values independently and uniformly from the range $[0,1]$.</li>
			<li>Otherwise, the model has a blendshape that controls both irises at the same time. So, I sample a value uniformly from $[0,1]$ and set the two iris size parameters to this value.</li>
		</ul> 
		</p>

		<p>Group 4, 5, and 6 are handled with the same algorithm. They all have multiple semantics, and I have to make sure that the weights of blendshapes in each group satisfy Equation (1) in Section 4. For each group, while all semantics can have non-zero parameter values, I only sample vectors where only one or two semantics have non-zero values for the sake of implementation simplicity.
		<ul> 
			<li>With probability $25\%$, I sample a vector with only one semantic having non-zero parameter value(s). I uniformly sample a semantic from the group and then sample one parameter value (or two if the semantic is two-sided) using a probability distribution that grows linearly from $0$ to $1$. (This is similar to <a href="http://pkhungurn.github.io/talking-head-anime/index.html#joint-parameter-distribution">Figure 5A</a> of the 2019 article but I only use the positive half this time.)</li>

			<li>With probability $75\%$, I sample a vector with two semantics having non-zero parameter values. I randomly sample the first semantic and then sample its parameter value(s) uniformly from the range $[0,1]$. Let us say the sampled value is $x$. I then uniformly sample another unselected semantic and sample its value(s) randomly from the range $[0,1-x]$.</li>
		</ul>
		</p>

		<p>After all the data have been sampled, I created 3D scenes according to the sampled specifications and rendered the input and ground truth images with the <a href="https://en.wikipedia.org/wiki/Phong_reflection_model">Phong reflection model</a>.</p>

		<h4>5.2.4 &nbsp; Generated Datasets</h4>

		<p>Following standard machine learning procedure, I created three datasets: training, validation, and test. I reused the 2019 article's collection of 3D models, which was already partitioned into three subsets in order to generate non-overlapping data. However, I cleaned up the collection by removing even more duplicated models, so the numbers of models are slightly less this time. The datasets' statistics are given in the table below.</p>

		<table class="table table-striped">
			<tr class="table-dark">
				<td>&nbsp;</td>
				<td align="right"><b>Training set</b></td>
				<td align="right"><b>Validation set</b></td>
				<td align="right"><b>Test set</b></td>
			</tr>
			<tr>
				<td># models used</td>
				<td align="right">7,827</td>
				<td align="right">79</td>
				<td align="right">68</td>
			</tr>
			<tr>
				<td># examples</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td># input images</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td># $GT_a$ images</td>
				<td align="right">316,794</td>
				<td align="right">8,353</td>
				<td align="right">7,716</td>
			</tr>
			<tr>
				<td># $GT_b$ images</td>
				<td align="right">316,794</td>
				<td align="right">8,353</td>
				<td align="right">7,716</td>
			</tr>
			<tr>
				<td># $GT_c$ images</td>
				<td align="right">316,794</td>
				<td align="right">8,353</td>
				<td align="right">7,716</td>
			</tr>
			<tr>
				<td># $GT_d$ images</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td># $GT_e$ images</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td># $GT_f$ images</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td># $GT_g$ images</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">9,715</td>
			</tr>
			<tr>
				<td>Total number of images</td>
				<td align="right">3,450,382</td>
				<td align="right">75,059</td>
				<td align="right">71,723</td>
			</tr>
		</table>
		<center><b>Table 5.5</b> Statistics of the three generated datasets.</center>
		<br>

		<p>Note that there are fewer $GT_a$, $GT_b$, and $GT_c$ images than the number of examples. This is because there are some models whose eyebrows are not cleanly separated from the rest of the body. These models were identified and marked unusable using the tool in Section 5.2.2 <a href="#fn_unsuable_eyebrows">[footnote]</a>.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id=fn_unsuable_eyebrows>
					When all the three extra eyebrow rendering modes are usuable, I consider the model unsuable for rendering eyebrows.
				</li>
			</ul>
		</div>

		<a name="networks"></a>
		<h2>6 &nbsp; Networks</h2>

		<p>Recall that, in Section 3, the new face morpher is composed of two subnetworks: the eyebrow segmenter and the eye & mouth morpher. These networks and their subnetworks are all made from the same building blocks.</p>

		<h3>6.1 &nbsp; Common Building Blocks</h3>

		<p>Following <a href="https://pytorch.org/">PyTorch</a>'s convention, 2D features are represented by a $c \times h \times w$ tensor where
		<ul>
			<li>$c$ is the number of channels,</li>
			<li>$h$ is the height (the number of pixels in the vertical direction), and</li>
			<li>$w$ is the width (the number of pixels in the horizontal direction).</li>
		</ul>
		Hence, depending on the network under discussion, an input image is represented by a $c \times 128 \times 128$ tensor or a $c \times 192 \times 192$ tensor. Since $h$ is always equal to $w$ in this article, I shall use the symbol $s$ (for "image <b>s</b>ize") for both values. The pixel values are normalized so that they lie in the range $[-1,1]$.
		</p>

		<p>A pose vector is represented by vector with $k$ dimensions. Before being fed into the subnetworks, it will be converted into a $k \times 16 \times 16$ tensor or a $k \times 24 \times 24$ tensor by expanding each dimension into a 2D image.</p>

		<a name="encoder-decoder-network" />
		<h4>6.1.1 &nbsp; Encoder-Decoder Network</h4>

		<p>Each of the my subnetworks contains an encoder-decoder network with the same architecture. It receives a $c \times s \times s$ image tensor and a pose vector and outputs a $64 \times s \times s$ tensor that will be further processed differently by different subnetworks.</p>

		<p>The encoder-decoder network is built from the following units:
		<ul>
			<li>$\mathrm{Conv3}(\cdot)$ denotes a convolution with a $3 \times 3$ kernel with stride of 1 pixel and padding of 1 pixel. This convolution preserves the width and height but may change the number of channels.</li>

			<li>$\mathrm{ConvDown}(\cdot)$ denotes a convolution with a 4$\times$4 kernel with stride of 2 pixels and padding of 1 pixel. The convolution downsamples the input by a factory of 2.</li>

			<li>$\mathrm{ConvUp}(\cdot)$ denotes a <i>transposed</i> convolution with a 4$\times$4 kernel with stride of 2 pixels and padding of 1 pixel. It upsamples the input by a factory of 2.</li>

			<li>$\mathrm{InstNorm}(\cdot)$ denotes the instance normalization unit <a href="#fn_ulyanov_2016">[Ulyanov et al. 2016]</a>.</li>

			<li>$\mathrm{Relu}(\cdot)$ denotes the rectified linear unit <a href="#fn_glorot_2011">[Glorot et al. 2011]</a>.</li>

			<li>$\mathrm{ResNetBlock}(\cdot)$ denotes a residual block <a href="#fn_he_2016">[He et al. 2016]</a>. I use the version defined in Pumarola et al.'s paper <a href="#fn_pumarola_2018_4">[2018]</a>: 
			$$\mathrm{ResnetBlock}(X) = X + \mathrm{InstNorm}(\mathrm{Conv3}(\mathrm{Relu}(\mathrm{InstNorm}(\mathrm{Conv3}(X))))).$$
			Note that a residual block always preserves the size of the input.
			</li>
		</ul>
		The detailed architecture is given below.
		</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_ulyanov_2016">
					Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
					<b>Instance Normalization: The Missing Ingredient for Fast Stylization.</b>
					2016.
					<a href="https://arxiv.org/abs/1607.08022">[arXiv]</a>
				</li>
				<li class="footnote" id="fn_glorot_2011">
					Xavier Glorot, Antione Bordes, and Yoshua Bengio.
					<b>Deep Sparse Rectifier Neural Networks.</b>
					AISTATS 2011.
					<a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">[PDF]</a>
				</li>
				<li class='footnote' id="fn_he_2016">
	                Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
	                <b>Deep Residual Learning for Image Recognition.</b>
	                CVPR 2016.
	                <a href="https://arxiv.org/abs/1512.03385">[arXiv]</a>
	            </li>
	            <li class="footnote" id="fn_pumarola_2018_4">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
			</ul>
		</div>

	    <table class="table">
	        <thead class="table-dark">
	            <tr>
	                <th scope="col">Tensors</th>
	                <th scope="col">Shape</th>
	            </tr>
	        </thead>
	        <tbody>
	            <tr class="table-primary">
	                <td>
	                    $A_0 = $ input image tensor
	                </td>
	                <td>
	                    $c \times s \times s$
	                </td>
	            </tr>
	            <tr class="table-primary">
	                <td>
	                    $A_1 = $ pose vector
	                </td>
	                <td>
	                    $k$
	                </td>
	            </tr>
	            <tr class="table-primary">
	                <td>
	                    $A_2 = A_1$ converted to 2D feature tensor
	                </td>
	                <td>
	                    $k \times (s / 8) \times (s / 8)$
	                </td>
	            </tr>
	            <tr class="table-warning">
	                <td>
	                    $B_0 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{Conv3}(A_0)))$
	                </td>
	                <td>
	                    $64 \times s \times s$
	                </td>
	            </tr>
	            <tr class="table-warning">
	                <td>
	                    $B_1 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvDown}(B_0)))$
	                </td>
	                <td>
	                    $128 \times (s/2) \times (s/2)$
	                </td>
	            </tr>
	            <tr class="table-warning">
	                <td>
	                    $B_2 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvDown}(B_1)))$
	                </td>
	                <td>
	                    $256 \times (s/4) \times (s/4) $
	                </td>
	            </tr>
	            <tr class="table-warning">
	                <td>
	                    $B_3 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvDown}(B_2)))$
	                </td>
	                <td>
	                    $512 \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $C_0 = B_3$ concatenated with $A_2$
	                </td>
	                <td>
	                    $(512 + k) \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $C_1 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{Conv3}(C_0)))$
	                </td>
	                <td>
	                    $512 \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $C_2 = \mathrm{ResNetBlock}(C_1)$
	                </td>
	                <td>
	                    $512 \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $C_3 = \mathrm{ResNetBlock}(C_2)$
	                </td>
	                <td>
	                    $512 \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $\qquad\qquad\vdots$
	                </td>
	                <td>
	                    $\qquad\vdots$
	                </td>
	            </tr>
	            <tr class="table-success">
	                <td>
	                    $C_6 = \mathrm{ResNetBlock}(C_5)$
	                </td>
	                <td>
	                    $512 \times (s/8) \times (s/8)$
	                </td>
	            </tr>
	            <tr class="table-danger">
	                <td>
	                    $D_0 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvTransposeUp}(C_6)))$
	                </td>
	                <td>
	                    $256 \times (s/4) \times (s/4)$
	                </td>
	            </tr>
	            <tr class="table-danger">
	                <td>
	                    $D_1 = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvTransposeUp}(D_0)))$
	                </td>
	                <td>
	                    $128 \times (s/2) \times (s/2)$
	                </td>
	            </tr>
	            <tr class="table-danger">
	                <td>
	                    $D_{\mathrm{out}} = \mathrm{Relu}(\mathrm{InstNorm}(\mathrm{ConvTransposeUp}(D_1)))$
	                </td>
	                <td>
	                    $64 \times s \times s$
	                </td>
	            </tr>
	        </tbody>
	    </table>    
	    <p align="center"><b>Table 6.1</b> Architecture of the common encoder-decoder network.</p>

		<a name="image-features">
	    <h4>6.1.2 &nbsp; Image Features</h4>

	    <p>The feature tensor produced by the encoder-decoder network will be turned into several "image features" that will be used to transform input image. I use three types of image features.</p>

	    <p><b>Alpha mask.</b> An alpha mask is a one-channel image that where each pixel takes a real value in the range $[0,1]$. It is often used to perform "alpha blending:" 
	    \begin{align*}
	    	\mathrm{AlphaBlend}(\alpha, I_0, I_1) = (1 - \alpha) \times I_0 + \alpha \times I_1
	    \end{align*}
	    where $\alpha$ denotes an alpha mask, $I_0$ and $I_1$ denote two arbitrary images with the same resolution as the mask, and all arithmetic operations are pixel-wise operations.</p>

	    <p>
	    	<div align="center"><img src="data/face_morpher_steps/output_12.png"></div>
	    	<b>Figure 6.2</b> An alpha mask that highlights the eyelids of a character. Black is $0$, and white is $1$.
	    </p>

	    <p>An alpha mask is generated by passing the $64 \times s \times s$ feature tensor to a $3 \times 3$ convolution to produce an $1 \times s \times s$ tensor. The resulting tensor is then passed to a sigmoid activation function.
	    \begin{align*}
	    	\alpha = \mathrm{Sigmoid}(\mathrm{Conv3}(D_{\mathrm{out}}))
	    \end{align*}
	    where $D_{\mathrm{out}}$ is the output tensor from Table 6.1. When doing alpha blending of images with more than one channels, the alpha mask is <a href="https://pytorch.org/docs/stable/notes/broadcasting.html">broadcasted</a> to all the image channels.
	    </p>

	    <p><b>Image change.</b> An image change is a tensor with the same dimensionality as another image, usually the input image, that is added (after being processed in some way) to that image to change its pixels.</p>

	    <p>
	    	<div align="center"><img src="data/face_morpher_steps/output_13.png"></div>
	    	<b>Figure 6.3</b> An image change that meant to modify the eyelids of a character together with the alpha mask in Figure 6.2. Notice that most pixels are non-sensical except the ones in the areas to change.
	    </p>

	    <p>It is generated by passing the encoder-decoder's output to a $3 \times 3$ convolution and then a hyperbolic tangent activation:
	    \begin{align*}
	    	\Delta I = \mathrm{Tanh}(\mathrm{Conv3}(D_{\mathrm{out}})).
	    \end{align*}
	    Here, $\Delta I$ denotes the change to be applied to some image $I$.
	    </p>

		<a name="appearance-flow-offset">
	    <p><b>Appearance flow offset.</b> I use appearance flow to warp images. Recall that an appearance flow is a map that tells, for each pixel of an image, where in another image to copy pixels from. It is represented by an $2 \times h \times w$ tensor because a 2D position requires two coordinates. Following PyTorch's naming convention, let us denote the act of using an appearance to create a new image from an old one with the $\mathrm{GridSample}$ function:
	    \begin{align*}
	    	I_{\mathrm{new}} = \mathrm{GridSample}(I_{\mathrm{old}}, F).
	    \end{align*}
	    Here, $F$ denotes the appearance flow tensor. The <i>identity appearance flow</i> is a flow that, for each pixel, copies from that pixel's location, thereby regenerating the input image exactly. In other words,
	    \begin{align*}
	    	I = \mathrm{GridSample}(I, F_{\mathrm{identity}})
	    \end{align*}
	    for any image $I$.</p> 

	    <p>Because this article's networks only warp facial features and not the whole image, the appearance flows they generate would be close to the identity flow. As such, having them generate the flows directly from $D_{\mathrm{out}}$ would impose unnecessary burden on the networks. It would be easier to ask them to generate <b>appearance flow offsets</b> &mdash; how the desired flows are different from the identity flow &mdash; instead. In other words, we would pass $D_{\mathrm{out}}$ to a $3 \times 3$ convolution to generate a tensor $\Delta F$ of size $2 \times h \times w$:
	    \begin{align*}
	    	\Delta F = \mathrm{Conv3}(D_{\mathrm{out}}).
	    \end{align*}
	    Then, the desired appearance flow would be computed by adding the offset to the identity flow:
	    \begin{align*}
	    	F = F_{\mathrm{identity}} + \Delta F.
	    \end{align*}
	    </p>

	    <p>
	    	<div align="center"><img src="data/face_morpher_steps/output_18.png"></div>
	    	<b>Figure 6.4</b> An appearance flow offset that moves a character's irises and mouth. Each pixel's color represents the direction (i.e., to the north, to the east, etc.) of the change vector, and its intensity represents the magnitude. The color black means the flow at that pixel is not changed. Notice that the offset is black in most areas except the irises, the mouth, and the borders (which most likely contain contentless background pixels).
	    </p>	    

		<a name="image-transformation-units">
	    <h4>6.1.3 &nbsp; Image Transformation Units</h4>

	    <p>The image features are the base from which one can build reusable neural network units that transform images. I use three types of units.</p>

	    <p><b>Partial image change unit.</b> An alpha mask and an image change are create from the feature tensor $D_{\mathrm{out}}$. They are subsequently blended with the original image to produce the final output:
	    \begin{align*}
	    	\mathrm{output} = \mathrm{AlphaBlend}(\alpha, I, \Delta I).
	    \end{align*}
		In their 2018 paper <a href="#fn_pumarola_2018_5"></a>, Pumarola et al. introduce an architecture they call  "attention-based generator," which uses the above calculation to change a human subject's facial expression. As I will use the calculation above several times, I shall give it a new name of <b>partial image change</b>. This is done to emphasize that it is a reusable unit of a neural network rather than the whole network like the term "attention-based generator" would imply. The word "partial" signifies the role of the alpha mask: it selects which parts of the original image to keep and which parts to change.
	    </p>

	    <div class="footnotes">
			<ul>				
	            <li class="footnote" id="fn_pumarola_2018_5">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
			</ul>
		</div>

		<p>			
			<table align="center">
				<tr>
					<td>
						<a href="data/partial_image_change_unit.png"><img src="data/partial_image_change_unit.png" width="600"></a>
					</td>
				</tr>
				<tr>
					<td align="center"><b>Figure 6.5</b> The partial image change unit.</td>
				</tr>
			</table>			
		</p>

		<p><b>Combining unit.</b> The combining unit constructs an alpha mask from the feature tensor and then uses it to combine two images through alpha blending.</p>

		<p>			
			<table align="center">
				<tr>
					<td>
						<a href="data/combining_unit.png"><img src="data/combining_unit.png" width="600"></a>
					</td>
				</tr>
				<tr>
					<td align="center"><b>Figure 6.6</b> The combining unit.</td>
				</tr>
			</table>			
		</p>

		<p><b>Warping unit.</b> An appearance flow offset is constructed from the feature tensor and then added to the identity appearance flow. The resulting flow is then used to warp the input image.</p>

		<p>			
			<table align="center">
				<tr>
					<td>
						<a href="data/warping_unit.png"><img src="data/warping_unit.png" width="600"></a>
					</td>
				</tr>
				<tr>
					<td align="center"><b>Figure 6.7</b> The warping unit.</td>
				</tr>
			</table>			
		</p>

	    <p>Now that all the building blocks have been described, we are now ready to discuss the subnetworks' architectures.</p>
	    
		<h3>6.2 &nbsp; Eyebrow Morpher</h3>

		<p>The eyebrow morpher consists of two subnetworks, the eyebrow segmenter and the eyebrow warper, that are trained jointly.</p>

		<a name="eyebrow-segmenter"></a>
		<h4>6.2.1 &nbsp; Eyebrow Segmenter</h4>

		<p>The segmenter takes in a $128 \times 128$ RGBA image. It uses the encoder-decoder in Section 6.1.1 to produce a $64 \times 128 \times 128$ feature tensor. (However, the encoder-decoder does not take a pose vector as input, so the tensors $A_1$ and $A_2$ in Table 6.1 are empty tensors.) The feature is then used to perform two partial image changes. One extracts the eyebrows from the input, and the other removes them. 
		</p>
	</div>

		<p align="center">
			<a href="data/eyebrow_segmenter.png"><img src="data/eyebrow_segmenter.png" width="800"></a><br>
			<b>Figure 6.8</b> Architecture of the eyebrow segmenter.
		</p>		

	<div class="container" style="max-width: 640px">
		<p>Note that there is a subtlety in the architecture above. The top alpha blending node (the one marked with a *) is different from the others. It represents the calculation
		\begin{align*}
			\mathrm{AlphaBlend^*}(\alpha, I, \Delta I) = (1 - \alpha) \times \Delta I + \alpha \times I
		\end{align*}
		where the alpha mask tells what pixels to copy from the input image $I$. However, normally, an alpha blending node would stand for the calculation
		\begin{align*}
			\mathrm{AlphaBlend}(\alpha, I, \Delta I) = (1 - \alpha) \times I + \alpha \times \Delta I
		\end{align*}
		where the alpha mask tells what pixels in the input image $I$ to change.
		</p>

		<a name="eyebrow-warper"></a>
		<h4>6.2.2 &nbsp; Eyebrow Warper</h4>

		<p>The eyebrow warper takes in the outputs of the eyebrow segmenter and concatenates them to form a $8 \times 128 \times 128$ image tensor. Again, the tensor is then passed to an encoder-decoder together with a $12$-dimensional eyebrow pose vector to produce a $64 \times 128 \times 128$ feature tensor, which is then used to:
		<ul>
			<li>warp the eyebrows,</li>
			<li>perform a partial image change on the the warped eyebrows to increase its quality, and</li>
			<li>combine the warped eyebrows back to the face.</li>
		</ul>
		</p>

		<p>While the final output is the face with warped eyebrows, the retouched eyebrow image will be used in the training process.</p>
	</div>

		<p align="center">
			<a href="data/eyebrow_warper.png"><img src="data/eyebrow_warper.png" width="960"></a><br>
			<b>Figure 6.9</b> Architecture of the eyebrow warper.
		</p>

	<div class="container" style="max-width: 640px">
		<h4>6.2.3 &nbsp; Training</h4>

		<p>Before we can discuss the loss function, let us define some symbols.
		<ul>
			<li>Let $p_{\mathrm{data}}$ denote the probability distribution of the training examples.</li>

			<li>A training example is a tuple $(I, \mathbf{p}, GT_a, \dotsc, GT_f)$ where $I$ is the rest pose image, $\mathbf{p}$ is the pose vector, and $GT_a, \dotsc, GT_f$ are ground truth images as discussed in Section 5.2.1.</li>

			<li>Let $\mathrm{EBS}$ denote the eyebrow segmenter, and let $\mathrm{EBS}_i(\cdot)$ denote its $i$th output as a function of the input image $I$. Here, the first output $\mathrm{EBS}_1(\cdot)$ is the segmented eyebrows, and the second output $\mathrm{EBS}_2(\cdot)$ is the face with the eyebrows removed.</li>

			<li>Let $\mathrm{EBW}$ denote the eyebrow warper, and let $\mathrm{EBW}_i(\cdot, \cdot, \cdot)$ denote its $i$th output. The first output $\mathrm{EBW}_1$ is the warped and retouched eyebrow image. The second output $\mathrm{EBW}_2$ is the face with the warped eyebrow composited back.</li>
		</ul>		
		</p>

		<p></p>

		<p>The loss function for the eyebrow segmenter is given below:
		\begin{align*}
			\mathcal{L}_{\mathrm{EBS}} 
			&= E_{(I,\mathrm{p},GT_a,\dotsc,GT_f) \sim p_{\mathrm{data}}} 
			\bigg[ 
			32 \Big\| \mathrm{EBS}_1(I) - GT_b  \Big\|_1 
			+  \Big\| \mathrm{EBS}_2(I) - GT_a \Big\|_1 \bigg] \\
			& \qquad + E_{(I,\mathrm{p},GT_a,\dotsc,GT_f) \sim p_{\mathrm{data}}} 
			\bigg[ 128 \Big\| \mathbb{1}(|I - GT_a| > 10^{-3}) \odot (\mathrm{EBS}_2(I) - GT_a) \Big\|_1
			\bigg].
		\end{align*}
		Note that $\| \mathrm{EBS}_1(I) - GT_b  \|_1$ and $\| \mathrm{EBS}_2(I) - GT_a \|_1$ are just L1 differences between the outputs of the segmenter and their corresponding ground truths. The term on the second line is what I call a <i>diff-masked</i> L1 difference. The difference $\mathrm{EBS}_2(I) - GT_a$ is multipled pixel-wisely (denoted by the $\odot$ operator) by a mask, $\mathbb{1}(|I - GT_a| > 10^{-3})$ before the L1 norm is computed. Here, $\mathbb{1}(\cdot)$ is the <i>indicator function</i>
		\begin{align*}
			\mathbb{1}(\mbox{predicate}) = \begin{cases}
				1, & \mbox{the predicate is true}, \\
				0, & \mbox{otherwise}.
			\end{cases}
		\end{align*}
		So, $\mathbb{1}(|I - GT_a| > 10^{-3})$ is a binary image that is $1$ where the pixel difference between $I$ (normal face) and $GT_a$ (face with eyebrows removed) is essentially non-zero, i.e. at eyebrow pixels in $I$. The diff-masked difference thus focuses the network's attention on the pixels that are to be removed. I set the coefficients of the terms to make the network focus on the eyebrows more than the whole image. The particular values, however, are arbitrary.
		</p>

		<p>The loss function for the eyebrow warper is just the L1 differences between the outputs and the ground truths:
		\begin{align*}
			\mathcal{L}_{\mathrm{EBW}} 
			&= E_{(I,\mathrm{p},GT_a,\dotsc,GT_f) \sim p_{\mathrm{data}}} 
			\bigg[ 
			\Big\| \mathrm{EBW}_1\Big(\mathrm{EBS}_1(I),  \mathrm{EBS}_2(I), \mathbf{p}\Big) - GT_c \Big\|_1
			\bigg] \\
			&\qquad + E_{(I,\mathrm{p},GT_a,\dotsc,GT_f) \sim p_{\mathrm{data}}} 
			\bigg[ \Big\| \mathrm{EBW}_2\Big(\mathrm{EBS}_1(I),  \mathrm{EBS}_2(I), \mathbf{p}\Big) - GT_d \Big\|_1 \bigg].
		\end{align*}
		</p>

		<p>I trained the network in two phases.
		<ul>
			<li>In the first phase, I trained only the eyebrow segmenter using $\mathcal{L}_{\mathrm{EBS}}$ for 12 epochs.</li>

			<li>In the second phase, I trained the eyebrow segmenter and the eyebrow warper together using the combined loss $\mathcal{L}_{\mathrm{EBS}} + \mathcal{L}_{\mathrm{EBW}}$ for 12 epochs.</li>
		</ul>
		In both phases, I used only the 316,794 training examples having all ground truths from $GT_a$ to $GT_d$. I used the Adam optimizer <a href="#fn_kingma_2015">[Kingma and Ba 2015]</a> with $\beta_1 = 0.5$ and $\beta_2 = 0.999$. The learning rate was $10^{-4}$, and the batch size was $20$. Training took about 2 days with a <a href="https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx/">Titan RTX</a> graphics card.
		</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_kingma_2015">
					Diederik P. Kingma and Jimmy Ba.
					<b>Adam: A Method for Stochastic Optimization.</b>
					ICRL 2015.
					<a href="https://arxiv.org/abs/1412.6980">[arXiv]</a>
				</li>
			</ul>
		</div>

		<a name="eye-mouth-morpher"></a>
		<h3>6.3 &nbsp; Eye & Mouth Morpher</h3>

		<p>The eye & mouth morpher takes as input a $192 \times 192$ image and a $25$-dimensional pose vector containing parameters for the eyes and the mouth (i.e., those in Table 4.5 and 4.6). From these inputs, it uses the encoder-decoder network to produce a feature tensor, which is in turn used to:
		<ul>
			<li>warp the input image to deform the mouth and the irises,</li>
			<li>perform a partial image change to retouch the mouth and irises, and</li>
			<li>perform another partial image change to deform the eyelids.</li>
		</ul>
		As we will see later in Section 7.2, the appearance flow in the first step helps preserve details of the irises, and the partial image change in the third step reduces artifacts on the closed eyelids.
		</p>
	</div>

		<p align="center">
			<a href="data/eye_and_mouth_morpher.png"><img src="data/eye_and_mouth_morpher.png" width="960"></a><br>
			<b>Figure 6.10</b> Architecture of the eye & mouth morpher.
		</p>

	<div class="container" style="max-width: 640px">		

		<p>Let us denote the eye & mouth morpher by $\mathrm{EMM}$. The network produces two outputs that are used in the training process. $\mathrm{EMM}_1(\cdot, \cdot)$ is the image with the mouth and irises morphed, and $\mathrm{EMM}_2(\cdot, \cdot)$ is the image with all facial features morphed.</p>

		<p>The loss function for the network is given by:
		\begin{align*}
			\mathcal{L}_{\mathrm{EMM}} 
			&= \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}\big(\mathrm{EMM}_1(GT_d,\mathbf{p}), GT_e\big) + \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}\big(\mathrm{EMM}_2(GT_d,\mathbf{p}), GT_f\big)
		\end{align*}
		where
		\begin{align*}
			\mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(X,Y) &= E_{(I,\mathrm{p},GT_a,\dotsc,GT_f) \sim p_{\mathrm{data}} } \Big[ \| X - Y\|_1 + \| \mathrm{Sobel}(X) - \mathrm{Sobel}(Y)\|_1 \Big].
		\end{align*}
		The second term inside the expectation is the L1 difference between the arguments after they are passed through the <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel operator</a>, which is used for edge detection. Accordingly, the loss not only penalizes differences between the output images and the corresponding ground truths but also differences between their "edge images." My aim for the Sobel term was to make the network pay more attention to edges as they are very important in anime-style drawings.
 		</p>

 		<p>I augmented the training by randomly scaling and translaing all images in each training example by the same amount before using them. This agumentation is very important to reduce the network's overfitting to the specification that the face must lie in the center $128 \times 128$ box. Again, I trained the network for 12 epochs (6M examples) with the Adam optimizer, reusing all the settings of the eyebrow morpher. Training took about 2 days with a Titan RTX graphics card.</p> 		

 		<h3>6.4 &nbsp; The Complete System</h3>

 		<p>Note that the architectures in this section are those of subnetworks of the new face morpher. In addition to modifying facial expression, the complete system also rotates the face, and this is carried out by the face rotator network as described in <a href="https://pkhungurn.github.io/talking-head-anime/">the 2019 article</a>. As a result, the whole system unfortunately became larger. It is now made of 5 big subnetworks <a href="#fn_subnetwork_list">[footnote]</a> instead of 3, and the size expanded from around 360MB to around 600MB. </p>

 		<p>Note that I use the two-algo face rotator and the combiner exactly as described in my previous article. They were trained with the old dataset in which facial expressions are not as rich as the new one. However, since the two networks were trained not to change facial expressions at all, they work well with the outputs of the new face morpher.</p>

 		<div class="footnotes">
 			<ul>
 				<li class="footnote" id="fn_subnetwork_list">
 					The eyebrow segmenter, the eyebrow morpher, the eye & mouth morpher, the two-algo face rotator, and the combiner.
 				</li>
 			</ul>
 		</div>

 		<a name="results"></a>
		<h2>7 &nbsp; Results</h2>

		<h3>7.1 &nbsp; Comparison with Previous Works</h3>

		<p>My system solves the parameter-based posing problem, and its state-of-the-art methods are Pumarola et al.'s <a href="#fn_pumarola_2018_6">[2018]</a> and Ververas's and Zaferiou's <a href="#fn_ververas_2020_2">[2020]</a>. However, I use the former's architecture as a part of my network, so I will compare against it in Section 7.2, which is the ablation study. I did not compare against the latter because it requires a 3DMM to implement, and I did not have access to one <a href="#fn_anime_3dmm"></a>. I also could not find a publicly available implementation of the project.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_anime_3dmm">
					Researchers have made 3DMMs of human faces available, but I do now know of any that is designed to work with anime characters.
				</li>
				<li class="footnote" id="fn_pumarola_2018_6">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. 
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018.
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>				
				<li class="footnote" id="fn_ververas_2020_2">
	            	Evangelos Ververas and Stefanos Zafeiriou.
	            	<b>SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters.</b>
	            	Int J Comput Vis 128, 2629–2650 (2020). 
	            	<a href="https://link.springer.com/article/10.1007/s11263-020-01338-7">[Paper]</a>
				</li>
			</ul>
		</div>

		<p>In this section, I will instead compare my work against two recent methods for motion transfer that take very different approaches: the "Bringing Portraits to Life" system by Averbuch-Elor et al. <a href="#fn_averbuch_elor_2017_3">[2017]</a> and the "first-order motion model" by Siarohin et al. <a href="#fn_siarohin_2019_4">[2019]</a>. The first is a representative non-deep-learning technique that uses lightweight 2D warps, and comparing against it would shed light on the effectiveness of lightweight 2D warps on anime characters. The algorithm generates <a href="https://www.youtube.com/watch?v=X1E1aGIUEis">good looking results</a> from human photos and is relatively easy to implement. The second is a versatile, state-of-the-art deep-learning-based algorithm for image animation that has captured the public's imagination. Its source code is publicly available, so I could easy train the model with my dataset.</p>		

		<div class="footnotes">
			<ul>					
				<li class="footnote" id="fn_averbuch_elor_2017_3">Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. <b>Bringing Portraits to Life.</b> SIGGRAPH Asia 2017. <a href="http://cs.tau.ac.il/~averbuch1/portraitslife/index.htm">[Project]</a></li>
				<li class="footnote" id="fn_siarohin_2019_4">Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. <b>First Order Motion Model for Image Animation</b>. NeurIPS 2019. <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">[Project]</a></li>
				<div class="footnotes">				
			</ul>
		</div>

		<h4>7.1.1 &nbsp; Preparation</h4>

		<h5>7.1.1.1 &nbsp; Data</h5>		

		<p>Averbuch-Elor et al.'s system requires the ability to detect 68 human facial landmarks as outputted by the <a href="http://dlib.net/">Dlib</a> library. However, I do not know of any software package that can do so for anime characters. As a result, I manually marked 68 vertices of 8 models in the test dataset as facial landmarks. Hence, evaluation of Averbuch-Elor et al.'s system must be carried out using only images of these models.</p>

		<p align="center">
			<table cellpadding="2" align="center">
			
			<tr>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kizuna_ai/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/tokino_sora/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/akiyama_rentarou/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kongou_iroha/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			</tr>

			<tr>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kizuna_ai/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>	
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/tokino_sora/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/akiyama_rentarou/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kongou_iroha/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>			
			</tr>

			<tr>
				<td align="center">(a)</td>
				<td align="center">(b)</td>
				<td align="center">(c)</td>
				<td align="center">(d)</td>
			</tr>

			<tr>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kiso_azuki/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/yamato_iori/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kitakami_futaba/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/weatheroid_airi/face.png" style="margin-top: -72px; margin-left: -64px"> 
			</div>
			</td>			
			</tr>

			<tr>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kiso_azuki/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/yamato_iori/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/kitakami_futaba/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			<td>
			<div style="overflow: hidden; width: 128px; height: 128px; border-width: 1px; border-style: solid;"> 
				<img src="data/test_models_with_landmarks/weatheroid_airi/landmarks.png" style="margin-top: -72px; margin-left: -64px"> 
			</div> 
			</td>
			</tr>

			<tr>
				<td align="center">(e)</td>
				<td align="center">(f)</td>
				<td align="center">(g)</td>
				<td align="center">(h)</td>
			</tr>
			</table>
			<b>Figure 7.1</b> Character models with manually annotated landmarks. They are 
			(a) <a href="https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA">Kizuna AI</a> (© Kizuna AI), 
			(b) <a href="https://www.youtube.com/channel/UCp6993wxpyDPHUpavwDFqgg">Tokino Sora</a> (© Tokino Sora Ch.), 
			(c) <a href="https://www.youtube.com/channel/UCom8rCUQZP98SIXJzMwjrxw">Akiyama Rentarou</a> (© ひま食堂), 
			(d) <a href="https://www.youtube.com/channel/UCiGcHHHT3kBB1IGOrv7f3qQ">Kongou Iroha</a>, 
			(e) <a href="https://www.youtube.com/channel/UCmM5LprTu6-mSlIiRNkiXYg">Kiso Azuki</a>, 
			(f) <a href="https://www.youtube.com/channel/UCyb-cllCkMREr9de-hoiDrg">Yamato Iori</a>, 
			(g) <a href="https://www.youtube.com/channel/UC5nfcGkOAm3JwfPvJvzplHg">Kitakami Futaba</a> (© Appland, Inc.), 
			and (h) <a href="https://www.youtube.com/channel/UCzrw4K7D9Ti3FP8WMTVPImg">Weatheroid Airi</a> (© Weathernews Inc.).
		</p>
		
		<h5>7.1.1.2 &nbsp; Implementing Averbuch-Elor et al.'s System</h5>		

		<p>I implemented Averbuch-Elor et al.'s algorithm by modifying an <a href="https://github.com/ekkravchenko/livePortraits">implementation by Ekaterina Kravchenko</a>, which uses <a href="https://opencv.org/">OpenCV</a> to perform most image processing subtasks. I changed the code to use manually annotated landmarks rather than those detected by Dlib and reimplemented the lightweight 2D warping code to improve visual quality. However, I simplified the system by, instead of inpainting the mouth into the target image, pasting the mouth pixels in directly. The main reason for this was that I already observed bigger problems caused by the algorithm that it became pointless to pursue a perfect reimplementation. I also omitted the fine-details-transferring step <a href="#fn_liu_2001">[Liu et al. 2001]</a> because anime-style characters generally do not have cast shadows or wrinkles that must be transferred.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_liu_2001">
					Zicheng Liu, Ying Shan, and Zhengyou Zhang.
					<b>Expressive Expression Mapping with Ratio Images.</b>
					SIGGRAPH 2001.
					<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/Expression-Mapping.pdf">[PDF]</a>
				</li>
			</ul>
		</div>

		<h5>7.1.1.3 &nbsp; Implementing Siarohin et al.'s System</h5>

		<p>I modified the <a href="https://github.com/AliaksandrSiarohin/first-order-model">code</a> published by the authors so that the model can be trained with my dataset. I trained three different models:
		<ol>
			<li>The <b>one-stop model</b> receives the rest pose image $I$ and $GT_g$ as inputs during training. As a result, it both modifies facial expression and rotates the face in one step.</li>
			<li>The <b>expression-only model</b> receives $I$ and $GT_f$ as inputs during training. It thus can only modify facial expression and does not know how to rotate the face.</li>
			<li>The <b>rotation-only model</b> receives $GT_f$ and $GT_g$ as inputs during training. It thus can only rotates the face and does not know how to modify facial expression.</li>
		</ol>
		All models uses 32 keypoints. I stopped training after the models have seen around 6M examples in order to match the amount used to train my networks. The learning rate for all the subnetworks was $2 \times 10^{-4}$, and it was scaled by a factor of $0.1$ once the models had seen around 3.5M examples and 4.75M examples. The batch size was 8.</p>

		<p>From the trained models, I created two systems for motion transfer.
		<ul>
			<li>The <b>one-step system</b> uses the one-stop model to transfer motion in one step. It thus requires two inputs: a source video a the target image.</li>
			<li>The <b>two-steps system</b>, on the other hand, performs pose transfer in two steps. It first transfers facial expression using the expression-only model and then transfers facial rotation using the rotation-only model. As a result, in addition to a source video and a target image, it requires another source video where facial expression changes, but head orientation does not.</li>
		</ul>
		Note that the two-steps system is not practical because it would be hard to prepare two such source videos in real usage scenarios. Its purpose is instead to allow me to make a fairer comparison between Siarohin et al.'s work and mine. Because my system has a face morpher trained specifically to change facial expression in a separate step, it would be unfair to compare it to the one-step system which is tasked to change facial expression and rotate the face all at once.
		</p>

		<h4>7.1.2 &nbsp; Qualitative Comparisons</h4>

		<p>I created a motion that has a character change its facial expression 10 times and used all systems to generate videos of the 8 characters in Figure 7.1. My system can generate videos directly from the motion. However, Averbuch-Elor et al.'s and Siarohin et al.'s system are motion transfer systems, so they need a source video to transfer motion from. (The two-steps variant of Siarohin et al.'s system needs two videos.) To this end, I used the ground truth renderings of the 4 models on the top row of Figure 7.1 as source videos, resulting in $4 \times 8$ videos being generated for each of the variants of these systems. The videos are available in Figure 7.2, and sample frames, chosen to illustrate differences between the systems, are given in Figure 7.3.</p>
	</div>

	<div align="center">
		<table border="1" cellpadding="5">
		<tr>
		<td id="compareOtherVideoCell">
			<video muted controls loop width="1080">
			    <source src="data/compare/other/video/motion_00/kizuna_ai/video.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="compareOtherTargetSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="kongou_iroha">Kongou Iroha</option>
			<option value="kiso_azuki">Kiso Azuki</option>
			<option value="yamato_iori">Yamato Iori</option>
			<option value="kitakami_futaba">Kitakami Futaba</option>
			<option value="weatheroid_airi">Weatheroid Airi</option>
		</select>
		</td>
		</tr>
		</table>
		<b>Figure 7.2</b> Videos generated by Averbuch-Elor et al.'s, Siarohin et al.'s, and my system. You can choose the target character with the combo box.
		<br>
		<br>
	</div>

	<script type="text/javascript">
		function changeCompareOtherTarget() {
			var newTarget = $("#compareOtherTargetSelect").val();
			var videoFileName = "data/compare/other/video/motion_00/" + newTarget + "/video.mp4";
			var html = "<video muted controls loop width=\"1080\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#compareOtherVideoCell").html(html);
		}

		$("#compareOtherTargetSelect").change(changeCompareOtherTarget);
	</script>
	
	<div align="center">
		<table cellpadding="5" id="compareOtherFrameTable">
			<tr><td></td></tr>			
		</table>
		<b>Figure 7.3</b> Samples of frames generated by the two previous works and my system.
		<br>
		<br>
	</div>
	
	<script type="text/javascript">
		function addCompareOtherLabel(html) {
			html += "<tr>";
			var firstRowOpenTd = "<td width='150' align='center' valign='bottom'><font size='2'><b>";
			var firstRowCloseTd = "</b></font></td>"
			html += firstRowOpenTd + "Source frame" + firstRowCloseTd;
			html += firstRowOpenTd + "Ground truth" + firstRowCloseTd;
			html += firstRowOpenTd + "Averbuch-Elor et al." + firstRowCloseTd;
			html += firstRowOpenTd + "Siarohin et al.<br>(1 step)" + firstRowCloseTd;
			html += firstRowOpenTd + "Siarohin et al.<br>(2 steps)" + firstRowCloseTd;
			html += firstRowOpenTd + "My system" + firstRowCloseTd;
			html += "</tr>"
			return html;
		}

		var html = "";

		html = addCompareOtherLabel(html);

		var columnNames = [
			"source",
			"ground_truth",
			"bptl",			
			"fomm_01",
			"fomm_02_03",
			"mode_13"
		];

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kongou_iroha/kitakami_futaba/" + columnName + "/00000090.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kongou_iroha/kitakami_futaba/" + columnName + "/00000090.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="300" height="300"'
			 	+ ' x="0" y="-80"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kongou_iroha/kitakami_futaba/" + columnName + "/00000090.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="300" height="300"'
			 	+ ' x="-120" y="-80"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kongou_iroha/kitakami_futaba/" + columnName + "/00000090.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="400" height="400"'
			 	+ ' x="-120" y="-180"/>';
			html += "</svg>"
			html += "</td>";
		}
		html += "</tr>"

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "When the source frame has head rotation, Averbuch-Elor et al\'s system produced a non-sensical result. Siarohin et al.'s networks yielded artifacts over the head probably because they tried to transfer the red ribbons to the target character. Moreover, they did not preserve the details of the irises while my system did.";
		html += '</font></td></tr>';

		html = addCompareOtherLabel(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/akiyama_rentarou/weatheroid_airi/" + columnName + "/00000020.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/akiyama_rentarou/weatheroid_airi/" + columnName + "/00000020.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="400" height="400"'
			 	+ ' x="-125" y="-180"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"		

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/akiyama_rentarou/" + columnName + "/00000060.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/akiyama_rentarou/" + columnName + "/00000060.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="400" height="400"'
			 	+ ' x="-125" y="-190"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "The results of these two frames show that performance of the two previous systems depends on similarity between the source and target characters. In the top frame, they did not completely close the eyes because the source's eyes are vertically shorter than the target's. In the bottom frame, they did not preserve the target eyes' shapes. My system does not have this problem because its operation is based on the pose vector.";
		html += '</font></td></tr>';


		html = addCompareOtherLabel(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/kizuna_ai/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="150" height="150"'
			 	+ ' x="0" y="0"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/kizuna_ai/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-420" y="-340"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/kizuna_ai/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="600" height="600"'
			 	+ ' x="-300" y="-290"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/kizuna_ai/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-350" y="-480"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/compare/other/frame/motion_00/kizuna_ai/kizuna_ai/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-325" y="-725"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "Even when the source and the target character is the same, Siarohin et al.'s networks can fail to reproduce important small details such as eyebrow, mouth shape, and patterns on the irises. The two-steps variant also blurred parts that are far from the face. While Averbuch-Elor et al.'s system was the best at transferring the mouth, it failed at rotating the face. My system rotated the face sensibly and reproduced small details well.";
		html += '</font></td></tr>';

		$("#compareOtherFrameTable").html(html);
	</script>

	<div class="container" style="max-width: 640px;">
		<p>From the videos and the sample frames, it can be seen that my system produces consistently good animation with few artifacts. Averbuch-Elor et al.'s system, on the other hand, was not designed to handle face rotation, so it deformed the face in non-sensical ways when faces are rotated. Siarohin et al.'s systems did not do well on facial organs. Both degraded details of the irises. The one-step system was particular bad at changing facial expression: the eyelids' and mouth's shapes lacked variation, and the eyebrows did not move. The two-steps system performed much better when the source and the target were the same, but it sometimes generated wrong eyebrow shapes. I also discovered that it significantly blurred the character's body, and the blurriness was caused by the expression-only model. (See Figure 7.4.) Another problem with Siarohin et al.'s systems is that they could yield artifacts around the head and deform the eyes in ways that do not respect the original shapes when the source was not similar to the target. In conclusion, the results show that both previous systems are not practical for generating character animation from a single image: they do not preserve details well, and their results' quality depends on similarity between the source and target characters.</p>

		<p>
			<table cellpadding="5" id="blurringTable"  align="center">
				<tr>
					<td align="center" valign="bottom">Ground truth</td>
					<td align="center" valign="bottom">Generated by<br>Siarohin et al.'s<br>expression-only model</td>
				</tr>
				<tr>
					<td>
						<svg width='256' height='216'>
							<rect x="0" y="0" width="256" height="256" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/ground_truth/00000080.png"
							width="256" height="256" x="0" y="-40"/>';
						</svg>						
					</td>
					<td>
						<svg width='256' height='216'>
							<rect x="0" y="0" width="256" height="256" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/fomm_02/00000080.png"
							width="256" height="256" x="0" y="-40"/>';
						</svg>						
					</td>
				</tr>
				<tr>
					<td>
						<svg width='256' height='128'>
							<rect x="0" y="0" width="256" height="128" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/ground_truth/00000080.png"
							width="512" height="512" x="-128" y="-256"/>';
						</svg>						
					</td>
					<td>
						<svg width='256' height='128'>
							<rect x="0" y="0" width="256" height="128" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/fomm_02/00000080.png"
							width="512" height="512" x="-128" y="-256"/>';
						</svg>						
					</td>
				</tr>
				<tr>
					<td>
						<svg width='256' height='128'>
							<rect x="0" y="0" width="256" height="128" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/ground_truth/00000080.png"
							width="512" height="512" x="-128" y="-384"/>';
						</svg>						
					</td>
					<td>
						<svg width='256' height='128'>
							<rect x="0" y="0" width="256" height="128" style="fill:black" />			
							<image xlink:href="data/compare/other/frame/motion_01/tokino_sora/tokino_sora/fomm_02/00000080.png"
							width="512" height="512" x="-128" y="-384"/>';
						</svg>						
					</td>
				</tr>
			</table>
			<b>Figure 7.4</b> An image generated by the expression-only model in Section 7.1.1.3 compared to the ground truth.
		</p>		

		<h4>7.1.3 &nbsp; Quantitative Comparisons</h4>

		<p>To evaluate the systems quantitatively, I ran them on two variants of the test dataset.
		<ul>
			<li>In the <i>full</i> variant, the ground truth image is $GT_g$, which has all expression changes and face rotation applied.</li>

			<li>In the <i>expression-only</i> variant, the ground truth image is $GT_f$. It has all expression changes, but the face is not rotated.</li>
		</ul>
		</p>

		<p>The previous systems were presented with the rest pose image $I$ and the corresponding ground truth image, and they were asked to transfer the pose of the ground truth image to $I$. <a href="#fn_two_steps_test">[footnote]</a> On the other hand, my system was given $I$ and the pose vector $\mathbf{p}$ and had to pose the character accordingly.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_two_steps_test">For the two-steps variant of Siarohin et. al.'s work, I presented it with three images: $I$, $GT_f$ and the corresponding grount truth image. I first used the expression-only model to transfer facial expression of $GT_f$, and then the rotation-only model to transfer face rotation of the ground truth image. Note that the ground truth image here can either be $GT_f$ or $GT_g$, depending on the variant of the test dataset under discusssion.</li>
			</ul>
		</div>

		<p>I compare the root mean squared error (RMSE) of pixel values and the structural similarity (SSIM) <a href="#fn_wang_2014_0">[Wang et al. 2004]</a> of their outputs with respect to the ground truth. Averages of these values over the test dataset are given below.</p>		

		<div class="footnotes">
	        <ul>
	            <li class="footnote" id="fn_wang_2014_0">
	                <p align="left">
	                    Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simoncelli.
	                    <b>Image Quality Assessment: From Error Visibility to Structural Similarity.</b>
	                    IEEE Transactions on Image Processing, Vol. 13, No 4, April 2004. <a href="https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf">[Paper]</a>
	                </p>
	            </li>
	        </ul>
	    </div>

	    <p>
	    	<table class="table table-striped">
	    		<tr class="table-dark">
	    			<td><b>Dataset</b></td>
	    			<td><b>System</b></td>
	    			<td align="right"><b>RMSE<br>(lower is better)</b></td>
	    			<td align="right"><b>SSIM<br>(higher is better)</b></td>
	    		</tr>
	    		<tr>
	    			<td rowspan="4">Full</td>
	    			<td >Averbuch-Elor et al. (*)</td>	
	    			<td align="right">0.118605</td>
	    			<td align="right">0.766838</td>
	    		</tr>
	    		<tr>	    			
	    			<td>Siarohin et al. (1 step)</td>
	    			<td align="right"><b>0.086136</b></td>
	    			<td align="right"><b>0.847905</b></td>
	    		</tr>
	    		<tr>
	    			<td>Siarohin et al. (2 steps)</td>
	    			<td align="right">0.094544</td>
	    			<td align="right">0.820290</td>
	    		</tr>	    		
	    		<tr>
	    			<td>Mine</td>
	    			<td align="right">0.102680</td>
	    			<td align="right">0.816101</td>
	    		</tr>
	    		<tr>	    			
	    			<td rowspan="4">Expression-only</td>
	    			<td >Averbuch-Elor et al. (*)</td>	
	    			<td align="right">0.047554</td>
	    			<td align="right">0.970536</td>
	    		</tr>	    		
	    		<tr>	    			
	    			<td>Siarohin et al. (1 step)</td>
	    			<td align="right">0.055287</td>
	    			<td align="right">0.933432</td>
	    		</tr>
	    		<tr>
	    			<td>Siarohin et al. (2 steps)</td>
	    			<td align="right">0.070527</td>
	    			<td align="right">0.890924</td>
	    		</tr>
	    		<tr>
	    			<td>Mine</td>
	    			<td align="right"><b>0.037855</b></td>
	    			<td align="right"><b>0.972168</b></td>
	    		</tr>	    		
	    	</table>
	    	<b>Table 7.5</b> Performance of Averbuch-Elor et al.'s, Siarohin et al.'s, and my systems with respect to the RMSE and SSIM metrics. (*) Note that Averbuch-Elor et al.'s system was evaluate with only the 8 models that have manually annotated landmarks, corresponding to 1,133 examples or around 10% of the dataset.
	    </p>

	    <p>The one-step variant of Siarohin et al.'s systems performed the best on the full variant of the dataset. This shows that their approach handled rotation transfer better than others thanks to its more sophisticated algorithm and also the fact that it was given the ground truth image as input. This result, however, does not tell the whole story because we saw earlier that the one-step system was bad at facial expressions. This fact is confirmed when we use all systems to transfer facial expression, in which case, my system performed the best. Hence, one may deduce that, because the one-stop model was trained to both change facial expression and rotate the face, it would spend most of its computational resources on getting the rotation right as doing would have more impact on the loss. Surprisingly, Averbuch-Elor et al. system's came in as the second perhaps because it copied the correct mouth from the ground truth image.</p>

	    <p>Another surprising fact is that the two-steps variant of Siarohin et al.'s systems ended up performing worse than the one-step variant on both datasets. This was the case despite the expectation that the expression-only model should improve the overall quality, especially on the expression-only dataset. Indeed, we saw in Figure 7.3 that it did improve facial expression transfer. However, we also saw in Figure 7.4 that it blurred locations that are far from movable parts, and so quality became worse in total. I surmise that Siarohin et al.'s approach might not work well when movements are concentrated on a small region of the image.</p>

	    <p>This section and the last show that <b>facial expression needs extra attention if we were to generate good facial animations.</b> My system produced those of the best overall quality because it had subnetworks designed to deform specific facial features.</p>

		<h3>7.2 &nbsp; Ablation Study</h3>

		<p>The architecture of the face morpher in Section 6 is quite complicated. There are three networks, and each has multiple image transformation steps. I settled on the present architecture after many design iterations. Here, I will compare it against three simpler architectures that I abandoned.</p>

		<h4>7.2.1 &nbsp; Baseline Architectures</h4>

		<p>All baseline architectures has only one network handle all facial features.</p>

		<h5>7.2.1.1 &nbsp; Partial Image Change Architecture</h5>

		The <b>partial image change</b> (PIC) architecture passes the inputs to the encoder-decoder in Section 5.1.1 and then follows it up with a partial image change step. Excluding minor implementation changes, it is the same as Pumarola et al.'s attention-based generator and the face morpher used in my previous article.
	</div>

		<p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/ablation/mode_14_architecture.png"><img src="data/ablation/mode_14_architecture.png" width="960"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 7.6</b> The partial image change architecture.
	                </td>
	            </tr>
	        </table>
	    </p>

	<div class="container" style="max-width: 640px">
	    <p>
	    Let $\mathrm{PIC}(\cdot, \cdot)$ denote the network's output. I trained the network with the following loss function
	    \begin{align*}
	    	\mathcal{L}_{\mathrm{PIC}} 
	    	&= \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(\mathrm{PIC}(I,\mathbf{p}),GT_f)
	   	\end{align*}
	   	for 12 epochs (6M examples). The settings for the optimizer are the same as those used for training the networks in Section 6.
	   	</p>

		<h5>7.2.1.2 &nbsp; Flow-Then-Change Architecture</h5>

		<p>The <b>flow-then-change</b> (FTC) architecture warps the input image and then performs a partial image change to retouch the result.</p>
	</div>

		<p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/ablation/mode_15_architecture.png"><img src="data/ablation/mode_15_architecture.png" width="960"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 7.7</b> The flow-then-change architecture.
	                </td>
	            </tr>
	        </table>
	    </p>

	<div class="container" style="max-width: 640px">
	    <p>The network was trained with the same loss function as the PIC network
	    \begin{align*}
	    	\mathcal{L}_{\mathrm{FTC}} 
	    	&= \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(\mathrm{FTC}(I,\mathbf{p}),GT_f)
	   	\end{align*}
	    for the same number of epochs using the same optimizer settings.</p>

		<h5>7.2.1.3 &nbsp; Three-Step Change Architecture</h5>

		<p>The <b>three-step change</b> (TSC) architecture deforms facial organs in the sequence that the face morpher in Section 6 does: the eyebrows, then the eyes and the mouth, and then the eyelids. However, the eyebrow step is not factored into independent networks and is implemented with warping followed by a partial image change.</p>
	</div>

		<p align="center">
	        <table align="center">
	            <tr>
	                <td align="center">
	                    <a href="data/ablation/mode_16_architecture.png"><img src="data/ablation/mode_16_architecture.png" width="960"></a>
	                </td>
	            </tr>
	            <tr>
	                <td align="center">
	                    <b>Figure 7.8</b> The three-step change architecture.
	                </td>
	            </tr>
	        </table>
	    </p>

	<div class="container" style="max-width: 640px">
	    <p>Let $\mathrm{TSC}_1(\cdot,\cdot)$ denote the face image with eyebrows morphed, $\mathrm{TSC}_2(\cdot,\cdot)$ the one with the eyes and mouth morphed, and $\mathrm{TSC}_3(\cdot,\cdot)$ the one with all features morphed. The network was trained with the following loss
	    \begin{align*}
	    	\mathcal{L}_{\mathrm{TSC}} 
	    	&= \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(\mathrm{TSC}_1(I,\mathbf{p}),GT_d) \\
	    	& \qquad + \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(\mathrm{TSC}_2(I,\mathbf{p}),GT_e) \\
	    	& \qquad + \mathcal{L}_{\mathrm{L1},\mathrm{Sobel}}(\mathrm{TSC}_3(I,\mathbf{p}),GT_f),
	   	\end{align*}
	   	using the same settings as the previous two baseline networks.
	    </p>

		<h4>7.2.2 &nbsp; Qualitative Comparisons</h4>

		<p>I reused the motion in Section 7.1.2 but removed the face rotation because I am comparing between networks that only change facial expressions. Using the four networks, I created a video for each of the characters in Figure 7.1, resulting in $4 \times 8 = 32$ videos which are shown in Figure 7.9. Samples of the frames are also given in Figure 7.10.</p>
	</div>

	<div align="center">
		<table border="1" cellpadding="5">
		<tr>
		<td id="ablationVideoCell">
			<video muted controls loop width="900">
			    <source src="data/ablation/video/motion_01/kizuna_ai/video.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="ablationCharacterSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="kongou_iroha">Kongou Iroha</option>
			<option value="kiso_azuki">Kiso Azuki</option>
			<option value="yamato_iori">Yamato Iori</option>
			<option value="kitakami_futaba">Kitakami Futaba</option>
			<option value="weatheroid_airi">Weatheroid Airi</option>
		</select>
		</td>
		</tr>
		</table>
		<b>Figure 7.9</b> Comparisons between videos generated by the new face morpher and the baseline networks.
		<br>
		<br>
	</div>

	<script type="text/javascript">
		function changeAblationCharacter() {
			var character = $("#ablationCharacterSelect").val();
			var videoFileName = "data/ablation/video/motion_01/" + character + "/video.mp4";
			var html = "<video muted controls loop width=\"900\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#ablationVideoCell").html(html);
		}

		$("#ablationCharacterSelect").change(changeAblationCharacter);
	</script>

	<div align="center">
		<table cellpadding="5" id="ablationFrameTable">
			<tr><td></td></tr>			
		</table>
		<b>Figure 7.10</b> Samples of frames generated by the new face morpher and the baseline networks.
		<br>
		<br>
	</div>
	
	<script type="text/javascript">
		function addAblationLabelRow(html) {
			html += "<tr>";
			var firstRowOpenTd = "<td width='150' align='center' valign='bottom'><font size='2'><b>";
			var firstRowCloseTd = "</b></font></td>"			
			html += firstRowOpenTd + "Ground truth" + firstRowCloseTd;
			html += firstRowOpenTd + "Partial Image Change" + firstRowCloseTd;
			html += firstRowOpenTd + "Flow-Then-Change" + firstRowCloseTd;
			html += firstRowOpenTd + "Three-Step Change" + firstRowCloseTd;
			html += firstRowOpenTd + "Section 6" + firstRowCloseTd;
			html += "</tr>"
			return html;
		}

		var html = "";

		var columnNames = [
			"ground_truth",
			"mode_14",
			"mode_15",
			"mode_16",
			"mode_13"
		];

		html = addAblationLabelRow(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/weatheroid_airi/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/weatheroid_airi/" + columnName + "/00000110.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-420" y="-420"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"		

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "(a) The PIC network deforms all facial features with a partial image change, losing high-frequency details of the irises. On the other hand, other networks have an appearance flow step to help preserve those details, so they performed much better.";
		html += '</font></td></tr>';

		html = addAblationLabelRow(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/yamato_iori/" + columnName + "/00000020.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/yamato_iori/" + columnName + "/00000020.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-420" y="-410"/>';
			html += '<rect x="16" y="10" width="35" height="28" fill="transparent" stroke="red">'
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"		

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "(b) Notice the small blemish over the left eyelid in the result of the flow-then-change image. This is caused by its using appearance flow to drag the eyelid down and so smearing the short line above it. Other networks use partial image change, which simply filled the eyelid with a solid color and yielded cleaner results.";
		html += '</font></td></tr>';

		html = addAblationLabelRow(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/kizuna_ai/" + columnName + "/00000102.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/kizuna_ai/" + columnName + "/00000102.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-420" y="-340"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"		

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "(c) The three baseline networks deform the eyebrows without segmenting them out first, and we can observe that they all generated blurry ones compared to the ground truth. The architecture in Section 6, on the other hand, yielded a much sharper result.";
		html += '</font></td></tr>';

		html = addAblationLabelRow(html);

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			html += "<td>";
			html += "<svg width='150' height='150'>"
			html += '<rect x="0" y="0" width="150" height="150" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/kongou_iroha/" + columnName + "/00000080.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="200" height="200"'
			 	+ ' x="-30" y="-25"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"

		html += "<tr>";
		for (var i = 0; i < columnNames.length; i++) {
			columnName = columnNames[i];
			if (columnName == "source") {
				html += "<td></td>"
				continue;
			}

			html += "<td>";
			html += "<svg width='150' height='75'>"
			html += '<rect x="0" y="0" width="150" height="75" style="fill:black" />';
			var fileName = "data/ablation/frame/motion_01/kongou_iroha/" + columnName + "/00000080.png";
			html += '<image xlink:href="' + fileName + '"' 
			 	+ ' width="800" height="800"'
			 	+ ' x="-420" y="-385"/>';
			html += "</svg>"
			html += "</td>";
		}
		html = html + "</tr>"		

		html += '<tr><td colspan="6" style="border: 1px solid black;"><font size="2">';
		html += "(d) The architecture of Section 6, however, can sometimes generate eyebrows that are darker and more prominent than those in the ground truth.";
		html += '</font></td></tr>';

		$("#ablationFrameTable").html(html);
	</script>

	<div class="container" style="max-width: 640px">

		<p>I first tried the PIC network because it is the same as the old face morpher. However, I observed that it did not preserve details of the irises as can be seen in Figure 7.10a. During the research for my 2019 article, I observed that having a network generate pixels from scratch tended to lose high-frequency details, and this was the reason why I rotated the face with two algorithms. I also noticed that the eyebrows could become blurry, especially when they were moved vertically up.</p>

		<p>To preserve the iris details, I added an warp to the PIC network, resulting in the FTC network. It did solve the loss of iris details problem but caused a new one: warping can smear small lines near the eyes. (See Figure 7.10b.) Moreover, the blurry eyebrow problem remained unsolved.</p>

		<p>With the TSC network, I added a partial image change step after the flow-then-change step and tasked it with specifically deforming the eyebrows, hoping that it would deform them cleanly like the PIC network did. I also added a flow-then-change step to specifically deform the eyebrows in an attempt to make them sharper. The TSC successfully solved the line-smearing problem, but the blurry eyebrows remained.</p>

		<p>The architecture in Section 6 evolved from the TSC network. I factored out the eyebrow morphing step into the eyebrow morpher. By using a separate network to segment, morph, and composite the eyebrows back to the face image, I introduced a strong bias to preserve the eyebrows' sharpness. As can been see in the last column of Figure 7.10c, this strategy worked well for this purpose, but the bias can be too strong in some cases. The eyebrows can have a darker color than that of the ground truth, and they can appear on top of hair bangs when they are supposed to be partially hidden underneath. (See Figure 7.10d.) Nevertheless, I think this is a good trade-off because prominent eyebrows are important for conveying the character's emotions.</p>

		<h4>7.2.3 &nbsp; Quantitative Comparisons</h4>

		<p>I ran the four architectures on the expression-only variant of the test dataset. Averages of RMSE and SSIM values are reported in the table below.</p>

		<p>
	    	<table class="table table-striped">
	    		<tr class="table-dark">
	    			<td><b>Architecture</b></td>
	    			<td align="right"><b>RMSE</b></td>
	    			<td align="right"><b>SSIM</b></td>
	    		</tr>
	    		<tr>
	    			<td>Partial image change</td>
	    			<td align="right">0.038112</td>
	    			<td align="right">0.972083</td>
	    		</tr>
	    		<tr>
	    			<td>Flow-then-change</td>
	    			<td align="right"><b>0.037526</b></td>
	    			<td align="right"><b>0.972545</b></td>
	    		</tr>
	    		<tr>
	    			<td>Three-step change</td>
	    			<td align="right">0.037548</td>
	    			<td align="right">0.972384</td>
	    		</tr>
	    		<tr>
	    			<td>Section 6</td>
	    			<td align="right">0.037855</td>
	    			<td align="right">0.972168</td>
	    		</tr>
	    	</table>
	    	<b>Table 7.11</b> Performance of the three baseline networks and the new face morpher of Section 6 according to the RMSE and SSIM metrics. I used the expression-only variant of the test dataset to compute the values.
	    </p>

	    <p>The metrics are close to one another because, as Figure 7.10 shows, differences between the outputs are subtle. The PIC network performed the worst perhaps because it did not preserve the irises' details. Section 6's architecture did worse than the FTC and TSC networks, a fact that might be attributable to its eyebrow bias. Here, we see yet again that the architecture that I ultimately use did not perform the best numerically. However, qualitative evaluation shows that it generates better results.</p>

		<h3>7.3 &nbsp; Application to Drawings</h3>

		<p>I applied my system to 200 images of VTubers and related characters to generate a short video clip for each, and I put all the videos together in the <a href="#eyecatcher">eyecatcher</a>. You can watch the individual videos in the figure below.</p>

		<p align="center">
			<table class="table" align="center" border="1">
				<tr>
					<td align="center">Image being animated</td>
					<td align="center">Video</td>
				</tr>
				<tr>
					<td align="center" valign="center" id="characterImageCell">
						<img src="data/characters/tsukino_mito/headshot.png">
					</td>
					<td align="center" valign="center" id="characterVideoCell">
						<video muted controls loop style="marrgin: 0 auto">
					    <source src="data/characters/tsukino_mito/video.mp4	" type="video/mp4">
					    </video>
					</td>
				</tr>
				<tr>
					<td align="center" colspan="2">
						<select id="characterSelect">
							<optgroup label="Nijisanji">
								<option value='aiba_uiha'>Aiba Uiha</option>
								<option value='aduchi_momo'>Aduchi Momo</option>
								<option value='aizono_manami'>Aizono Manami</option>
								<option value='akabane_youko'>Akabane Youko</option>
								<option value='amamiya_kokoro'>Amamiya Kokoro</option>
								<option value='amemori_sayo'>Amemori Sayo</option>
								<option value='ange_katrina'>Ange Katrina</option>
								<option value='ars_almal'>Ars Almal</option>
								<option value='asahina_akane'>Asahina Akane</option>
								<option value='asuka_hina'>Asuka Hina</option>
								<option value='dola'>Dola</option>
								<option value='belmond_banderas'>Belmond Banderas</option>
								<option value='eli_conifer'>Eli Conifer</option>
								<option value='elu'>Elu</option>
								<option value='emma_august'>Emma August</option>
								<option value='eudric'>Eudric</option>
								<option value='ex_albio'>Ex Albio</option>
								<option value='fumi'>Fumi</option>
								<option value='fumino_tamaki'>Fumino Tamaki</option>
								<option value='furen_e_lustario'>Furen E Lustario</option>
								<option value='fushimi_gaku'>Fushimi Gaku</option>
								<option value='fuwa_minato'>Fuwa Minato</option>
								<option value='gentsuki_toujirou'>Gentsuki Toujirou</option>
								<option value='gilzaren_iii'>Gilzaren III</option>
								<option value='gundou_mirei'>Gundou Mirei</option>
								<option value='gwelu_os_gar'>Gwelu Os Gar</option>
								<option value='hakase_fuyuki'>Hakase Fuyuki</option>
								<option value='hanabatake_chaika'>Hanabatake Chaika</option>
								<option value='harusaki_air'>Harusaki Air</option>
								<option value='hassaku_yuzu'>Hassaku Yuzu</option>
								<option value='hayama_marin'>Hayama Marin</option>
								<option value='hayase_sou'>Hayase Sou</option>
								<option value='higuchi_kaede'>Higuchi Kaede</option>
								<option value='honma_himawari'>Honma Himawari</option>
								<option value='hoshikawa_sara'>Hoshikawa Sara</option>
								<option value='ibrahim'>Ibrahim</option>
								<option value='ienaga_mugi'>Ienaga Mugi</option>
								<option value='inui_toko'>Inui Toko</option>
								<option value='izumo_kasumi'>Izumo Kasumi</option>
								<option value='joe_rikiichi'>Joe Rikiichi</option>
								<option value='kagami_hayato'>Kagami Hayato</option>
								<option value='kaida_haru'>Kaida Haru</option>
								<option value='kanae'>Kanae</option>
								<option value='kataribe_tsumugi'>Kataribe Tsumugi</option>
								<option value='kenmochi_touya'>Kenmochi Touya</option>
								<option value='kingyouzaka_meiro'>Kingyouzaka Meiro</option>
								<option value='kitakoji_hisui'>Kitakoji Hisui</option>
								<option value='kudou_chitose'>Kudou Chitose</option>
								<option value='kurusu_natsume'>Kurusu Natsume</option>
								<option value='kuzuha'>Kuzuha</option>
								<option value='levi_elipha'>Levi Elipha</option>
								<option value='lize_helesta'>Lize Helesta</option>
								<option value='luis_cammy'>Luis Cammy</option>
								<option value='machita_chima'>Machita Chima</option>
								<option value='maimoto_keisuke'>Maimoto Keisuke</option>
								<option value='makaino_lilim'>Makaino Lilim</option>
								<option value='mashiro'>Mashiro</option>
								<option value='matsukai_mao'>Matsukai Mao</option>
								<option value='mayuzumi_kai'>Mayuzumi Kai</option>
								<option value='melissa_kinrenka'>Melissa Kinrenka</option>
								<option value='moira'>Moira</option>
								<option value='mononobe_alice'>Mononobe Alice</option>
								<option value='morinaka_kazaki'>Morinaka Kazaki</option>
								<option value='nagao_kei'>Nagao Kei</option>
								<option value='nakao_azuma'>Nakao Azuma</option>
								<option value='naraka'>Naraka</option>
								<option value='nishizono_chigusa'>Nishizono Chigusa</option>
								<option value='nui_sociere_mouth_open'>Nui Sociere</option>
								<option value='onomachi_haruka'>Onomachi Haruka</option>
								<option value='otogibara_era'>Otogibara Era</option>
								<option value='ratna_petit'>Ratna Petit</option>
								<option value='rindou_mikoto'>Rindou Mikoto</option>
								<option value='ryuushen'>Ryuushen</option>
								<option value='saegusa_akina'>Saegusa Akina</option>
								<option value='sakura_ritsuki'>Sakura Ritsuki</option>
								<option value='sasaki_saku'>Sasaki Saku</option>
								<option value='seto_miyako'>Seto Miyako</option>
								<option value='setsuna'>Setsuna</option>
								<option value='shellin_burgundy'>Shellin Burgundy</option>
								<option value='shibuya_hajime'>Shibuya Hajime</option>
								<option value='shiina_yuika'>Shiina Yuika</option>
								<option value='shirayuki_tomoe'>Shirayuki Tomoe</option>
								<option value='shizuka_rin'>Shizuka Rin</option>
								<option value='sister_claire'>Sister Claire</option>
								<option value='sorahoshi_kirame'>Sorahoshi Kirame</option>
								<option value='sukoya_kana'>Sukoya Kana</option>
								<option value='suou_sango'>Suou Sango</option>
								<option value='suzuhara_lulu'>Suzuhara Lulu</option>
								<option value='suzuka_utako'>Suzuka Utako</option>
								<option value='suzuki_masaru'>Suzuki Masaru</option>
								<option value='suzuya_aki'>Suzuya Aki</option>
								<option value='takamiya_rion'>Takamiya Rion</option>
								<option value='todoroki_kyouko'>Todoroki Kyouko</option>
								<option value='toudou_kohaku'>Toudou Kohaku</option>
								<option value='tsukino_mito' selected="selected">Tsukino Mito</option>
								<option value='uduki_kou'>Uduki Kou</option>
								<option value='umiyasha_no_kami'>Umiyasha No Kami</option>
								<option value='ushimi_ichigo'>Ushimi Ichigo</option>
								<option value='warabeda_meiji'>Warabeda Meiji</option>
								<option value='yaguruma_rine'>Yaguruma Rine</option>
								<option value='yamagami_karuta'>Yamagami Karuta</option>
								<option value='yashiro_kizuku'>Yashiro Kizuku</option>
								<option value='yorumi_rena'>Yorumi Rena</option>
								<option value='yukishiro_mahiro'>Yukishiro Mahiro</option>
								<option value='yumeoi_kakeru'>Yumeoi Kakeru</option>
								<option value='yuuhi_riri'>Yuuhi Riri</option>
								<option value='yuuki_chihiro'>Yuuki Chihiro</option>
								<option value='yuzuki_roa'>Yuzuki Roa</option>
							</optgroup>
							<optgroup label="Hololive Production">
								<option value='a_chan'>A-Chan</option>
								<option value='airani_iofifteen'>Airani Iofifteen</option>
								<option value='akai_heart'>Akai Heart</option>
								<option value='aki_rosenthal'>Aki Rosenthal</option>
								<option value='amane_kanata_new_year'>Amane Kanata</option>
								<option value='anya_melfissa'>Anya Melfissa</option>
								<option value='arurandeisu'>Arurandeisu</option>
								<option value='astel_leda'>Astel Leda</option>
								<option value='ayunda_risu'>Ayunda Risu</option>
								<option value='gawr_gura'>Gawr Gura</option>
								<option value='hanasaki_miyabi'>Hanasaki Miyabi</option>
								<option value='himemori_ruuna'>Himemori Luna</option>
								<option value='hoshimachi_suisei'>Hoshimachi Suisei</option>
								<option value='houshou_marine'>Houshou Marine</option>
								<option value='inugami_korone'>Inugami Korone</option>
								<option value='kagami_kira'>Kagami Kira</option>
								<option value='kageyama_shien'>Kageyama Shien</option>
								<option value='kanade_izuru'>Kanade Izuru</option>
								<option value='kiryuu_koko'>Kiryuu Coco</option>
								<option value='kishidou_tenma'>Kishidou Tenma</option>								
								<option value='mano_aloe'>Mano Aloe</option>
								<option value='minato_aqua'>Minato Aqua</option>
								<option value='momosuzu_nene'>Momosuzu Nene</option>
								<option value='moona_hoshinova'>Moona Hoshinova</option>
								<option value='murasaki_shion'>Murasaki Shion</option>
								<option value='nakiri_ayame'>Nakiri Ayame</option>
								<option value='natsuiro_matsuri'>Natsuiro Matsuri</option>
								<option value='nekomata_okayu'>Nekomata Okayu</option>
								<option value='ninomae_inanis'>Ninomae Inanis</option>
								<option value='omaru_polka'>Omaru Polka</option>
								<option value='ookami_mio'>Ookami Mio</option>
								<option value='oozora_subaru'>Oozora Subaru</option>
								<option value='pavolia_reine'>Pavolia Reine</option>
								<option value='rikka'>Rikka</option>
								<option value='shirakami_fubuki'>Shirakami Fubuki</option>
								<option value='shiranui_flare'>Shiranui Flare</option>
								<option value='shirogane_noel'>Shirogane Noel</option>
								<option value='shishiro_botan'>Shishiro Botan</option>
								<option value='takanashi_kiara'>Takanashi Kiara</option>
								<option value='tokoyami_towa'>Tokoyami Towa</option>
								<option value='tsunomaki_watame'>Tsunomaki Watame</option>
								<option value='uruha_rusia'>Uruha Rushia</option>
								<option value='usada_pekora'>Usada Pekora</option>
								<option value='watson_amelia'>Watson Amelia</option>
								<option value='yozora_mel'>Yozora Mel</option>
								<option value='yukihana_ramii'>Yukihana Ramii</option>
								<option value='yuukoku_robel'>Yuukoku Robel</option>
								<option value='yuzuki_choko'>Yuzuki Choko</option>
							</optgroup>
							<optgroup label="774 Inc.">
								<option value='ginneko_nanashi'>Ginneko Nanashi</option>
								<option value='haineko_nanashi'>Haineko Nanashi</option>
								<option value='hashiba_natsumi'>Hashiba Natsumi</option>
								<option value='hinokuma_ran'>Hinokuma Ran</option>
								<option value='hira_hikari'>Hira Hikari</option>
								<option value='inaba_haneru'>Inaba Haneru</option>
								<option value='inari_kuromu'>Inari Kuromu</option>
								<option value='kazami_kuku'>Kazami Kuku</option>
								<option value='kojou_anna'>Kojou Anna</option>
								<option value='kuroneko_nanashi'>Kuroneko Nanashi</option>
								<option value='mikeneko_nanashi'>Mikeneko Nanashi</option>
								<option value='ryuugasaki_rin'>Ryuugasaki Rin</option>
								<option value='saionji_mary'>Saionji Mary</option>
								<option value='sekishiro_miko'>Sekishiro Mico</option>
								<option value='seshima_rui'>Seshima Rui</option>	
								<option value='shimamura_charlotte'>Shimamura Charlotte</option>
								<option value='shiromiya_mimi'>Shiromiya Mimi</option>
								<option value='shishio_chris'>Shishio Chris</option>
								<option value='soutsuki_eli'>Soutsuki Eli</option>
								<option value='souya_ichika'>Souya Ichika</option>
								<option value='suou_patra'>Suou Patra</option>
								<option value='umori_hinako'>Umori Hinako</option>
								<option value='yunohara_izumi'>Yunohara Izumi</option>
							</optgroup>
							<optgroup label="Noripro">
								<option value='beppy'>Beppy</option>
								<option value='enomiya_milk'>Enomiya Milk</option>
								<option value='himesaki_yuzuru'>Himesaki Yuzuru</option>
								<option value='houzuki_warabe'>Houzuki Warabe</option>
								<option value='inuyama_tamaki'>Inuyama Tamaki</option>
								<option value='kumatani_takuma'>Kumatani Takuma</option>
								<option value='shirayuki_mishiro'>Shirayuki Mishiro</option>
								<option value='yumeno_lilith'>Yumeno Lilith</option>
							</optgroup>
							<optgroup label="Reality">
								<option value='kmnz_lita'>KMNZ Lita</option>
								<option value='kmnz_liz'>KMNZ Liz</option>								
							</optgroup>							
							<optgroup label="Independent/Retired VTubers">
								<option value='anesaki_yukimi'>Anesaki Yukimi</option>
								<option value='chikuwa'>Chikuwa</option>
								<option value='fukuya_master'>Fukuya Master</option>
								<option value='hanagumo_kuyuri'>Hanagumo Kuyuri</option>
								<option value='hijirime_lyria'>Hijirime Lyria</option>
								<option value='itou_life'>Itou Life</option>
								<option value='kagura_mea'>Kagura Mea</option>
								<option value='kerin'>Kerin</option>
								<option value='magurona'>Magurona</option>
								<option value='natori_sana'>Natori Sana</option>
								<option value='natsume_eri'>Natsume Eri</option>
								<option value='shigure_ui'>Shigure Ui</option>									
								<option value='takehana_note'>Takehana Note</option>
								<option value='tenkai_tsukasa'>Tenkai Tsukasa</option>
								<option value='tomari_mari'>Tomari Mari</option>
								<option value='yuugasaki_umi'>Yuugasaki Umi</option>
								<option value='yuzuriha_honami'>Yuzuriha Honami</option>
							</optgroup>							
							<optgroup label="Related Characters">
								<option value='kyoumo_gyouza'>Kyoumo Gyouza</option>								
								<option value='mamaneru'>Mamaneru</option>
								<option value='mamatsuri'>Mamatsuri</option>
								<option value='natori_gilzaren'>Natori Gilzaren</option>
							</optgroup>
							<optgroup label="Waifu Labs">
								<option value='waifu_00'>Waifu #0</option>
								<option value='waifu_01'>Waifu #1</option>
								<option value='waifu_02'>Waifu #2</option>
								<option value='waifu_03'>Waifu #3</option>
								<option value='waifu_04'>Waifu #4</option>
							</optgroup>	
						</select>
					</td>
				</tr>
			</table>
			<div align="center"><b>Figure 7.12</b> Videos of characters being animated by my system.</div>			
		</p>		

		<script type="text/javascript">
		function changeCharacter() {
			var character = $("#characterSelect").val();
			var videoFileName = "data/characters/" + character + "/video.mp4";
			var imageFileName = "data/characters/" + character + "/headshot.png"			
			$("#characterImageCell").html('<img src="' + imageFileName + '">');
			$("#characterVideoCell").html(
				'<video muted controls loop style="marrgin: 0 auto;">'
				+ '<source src="' + videoFileName + '" type="video/mp4">'
				+ '</video>');
		}
		$("#characterSelect").change(changeCharacter);
		</script>

		<p>Below is a selection of characters making the 7 facial expressions in the abstract.</p>
	</div>

	<div align="center">
		<p>
			<table id="expressionTable">
				<tr>
					<td></td>
				</tr>				
			</table>
			<b>Figure 7.13</b> Facial expressions generated by my system. <a href="#fn_expression_figure_copyright">[copyright]</a>
		</p>
	</div>

	<div class="footnotes">
		<ul>
			<li class="footnote" id="fn_expression_figure_copyright">
				<p align="left">
					From top to bottom, the characters are:
					<ul style="text-align: left;">
						<li><a href="https://www.youtube.com/channel/UCD-miitqNY3nyukJ4Fnf4_A">Tsukino Mito</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCdn5BQ06XqgXoAxIhbqw5Rg">Shirakami Fubuki</a> (&copy; COVER Corp.)</li>
						<li><a href="https://www.youtube.com/channel/UCoztvTULBYd3WmStqYeoHcA">Sasaki Saku</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCqm3BQLlJfvkTsX_hvm0UmA">Tsunomaki Watame</a> (&copy; COVER Corp.)</li>
						<li><a href="https://www.ichikara.co.jp/news/210/">Eudric</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCKMYISTJAQ8xTplUPHiABlA">Yashiro Kizuku</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCHBhnG2G-qN0JrrWmMO2FTA">Shellin Burgundy</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCCzUftO8KOVkV4wQG1vkUvg">Houshou Marine</a> (&copy; COVER Corp.)</li>
						<li><a href="https://www.youtube.com/channel/UCZlDXzGoo7d44bwdNObFacg">Amane Kanata</a> (&copy; COVER Corp.)</li>
						<li><a href="https://www.youtube.com/channel/UCYTz3uIgwVY3ZU-IQJS8r3A">Shimamura Charlotte</a> (&copy; 774 Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCvzVB-EYuHFXHZrObB8a_Og">Yaguruma Rine</a> (&copy; Ichikara Inc.)</li>
						<li><a href="https://www.youtube.com/channel/UCt30jJgChL8qeT9VPadidSw">Shigure Ui</a> (&copy; Shigure Ui)</li>
						<li><a href="https://www.youtube.com/user/sihuto100">Fukuya Master</a> (&copy; Fukuya Master)</li>
						<li><a href="https://www.youtube.com/channel/UC_a1dZYZ8ZTXpjg9xUY9sj8w">Suzuhara Lulu</a> (&copy; Ichikara Inc.)</li>
					</ul>
				</p>
			</li>
		</ul>
	</div>

	<script type="text/javascript">
		function expressionImageCell(fileName) {
			var html = '<td><div style="width: 128px; height: 128px; overflow: hidden; border-width: 1px; border-style: solid;">';
			html += '<img style="margin-top: -32px; margin-left: -32px; width: 192px; height: 192px"';
			html += ' src="' + fileName + '">'
			html += '</div></td>';
			return html;
		}

		var html = "";		

		html += "<tr>";
		var headerOpening = '<td align="center"><font size="2">';
		var headerEnding = '</font></td>';
		html += headerOpening + 'Input' + headerEnding;
		html += headerOpening + 'Happy' + headerEnding;
		html += headerOpening + 'Sad' + headerEnding;
		html += headerOpening + 'Angry' + headerEnding;
		html += headerOpening + 'Disgusted' + headerEnding;
		html += headerOpening + 'Condescending' + headerEnding;
		html += headerOpening + 'Uwamedukai' + headerEnding;
		html += headerOpening + 'Gangimari-Gao' + headerEnding;
		html += "</tr>"

		var charactersToShow = [
			"tsukino_mito",
			"shirakami_fubuki",
			"sasaki_saku",
			"tsunomaki_watame",
			"eudric",
			"yashiro_kizuku",
			"shellin_burgundy",
			"houshou_marine",
			"amane_kanata_new_year",
			"shimamura_charlotte",
			"yaguruma_rine",
			"shigure_ui",
			"natori_sana",
			"fukuya_master",
			"suzuhara_lulu",
			"waifu_00"
		];

		for (var i=0;i<charactersToShow.length;i++) {
			html += '<tr>'
			var name = charactersToShow[i];
			html += expressionImageCell("data/characters/" + name + "/headshot.png");
			for (var j=1;j<8;j++) {
				html += expressionImageCell("data/characters/" + name + "/emotion/0000000" + j + ".png");
			}
			html += '</tr>'
		}

		$("#expressionTable").html(html);
	</script>

	<div class="container" style="max-width: 640px;">

		<p>The above figure demonstrates the versatility of my system. It could handle both male and female characters with very different eye and face shapes. It sensibly deformed the eyes even when partially occluded by hair or seen through translucent glasses. When the input image has an open mouth, it reuses the pixels to generate sharp mouths, and it hallucinates plausible mouth shapes otherwise.</p>
		
		<a name="limitations"/>
		<h3>7.4 &nbsp; Limitations</h3>

		<p>While the new face morpher added more controllable features, the system still has many limitations, most of which are inherited from the old system. They include:
		<ul>
			<li>The input face must be in the rest post, looking straight at the viewer with eyes open. It must also be contained in the middle $128 \times 128$ box.</li>
			<li>The input image must not have a background, and its alpha channel must indicate where the character is.</li>
			<li>Disoccluded parts of the rotate face can be blurry, and we saw that the rotation accuracy is worse than that of Siarohin et al's system.</li>
			<li>The size of the whole system has gotten even bigger (from 360M to 600M), and so it becomes slower. It thus requires a more powerful GPU to run in real time <a href="#fn_gpu">[footnote]</a> and cannot be used effectively on mobile devices.</li>
		</ul>
		Moreover, as with many machine learning systems, my system fails when it is given data that do not resemble the training set.</p>

		<div class="footnotes">
			<ul>
				<li class="footnote" id="fn_gpu">
					I developed the old system with a GeForce 1080 Ti, but I used the Titan RTX for the new system. 
				</li>
			</ul>
		</div>

		<p>Because almost all MMD models I collected have yellow or brown skins, the system cannot handle characters with uncommon skin colors.</p>

		<p>
			<table align="center">
				<tr><td>
					<video muted controls loop>
					<source src="data/characters/gilzaren_iii/expression_only_video.mp4" type="video/mp4">
					</video>
				</td></tr>
				<tr><td align="center">
					<a href="https://www.youtube.com/channel/UCUzJ90o1EjqUbk2pBAy0_aw">Gilzaren III</a> (&copy; Ichikara Inc.)
				</td></tr>
			</table>
			<b>Figure 7.14</b> Gilzaren III's skin is purple, and my system could not effectively deform his facial features, especially his mouth.
		</p>

		<p>I also observed that my system did not perform well on characters with strong makeups such as thick lipsticks and eye contours.</p>

		<p>
			<table align="center">
				<tr>
					<td>
						<video muted controls loop>
							<source src="data/characters/joe_rikiichi/expression_only_video.mp4" type="video/mp4">
						</video>
					</td>
					<td>
						<video muted controls loop>
							<source src="data/characters/hanabatake_chaika/expression_only_video.mp4" type="video/mp4">
						</video>
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.youtube.com/channel/UChUJbHiTVeGrSkTdBzVfNCQ">Joe Rikiichi</a> (&copy; Ichikara Inc.)
					</td>
					<td align="center">
						<a href="https://www.youtube.com/channel/UCsFn_ueskBkMCEyzCEqAOvg">Hanabatake Chaika</a> (&copy; Ichikara Inc.)
					</td>
				</tr>
			</table>
			<b>Figure 7.15</b> While my system could somewhat open the characters' mouths, the inner mouth are not clearly visible. It could not close one of Rikiichi's eyes, and it thought that the pink eyeshadows were part of Hanabatake's eyebrows.
		</p>

		<p>While the eyebrows of most anime characters are drawn as black lines or thin colored strips, some have oval shapes. As there are very few of these so called "maro" eyebrows (<a href="https://dic.pixiv.net/a/%E9%BA%BB%E5%91%82%E7%9C%89">麻呂眉</a>) in the training dataset, my system cannot deform them correctly.</p>

		<p>
			<table align="center">
				<tr>
					<td>
						<video muted controls loop>
							<source src="data/characters/usada_pekora/expression_only_video.mp4" type="video/mp4">
						</video>
					</td>
					<td>
						<video muted controls loop>
							<source src="data/characters/aizono_manami/expression_only_video.mp4" type="video/mp4">
						</video>
					</td>
				</tr>
				<tr>
					<td align="center">
						<a href="https://www.youtube.com/channel/UC1DCedRgGHBdm81E1llLhOQ">Usada Pekora</a> (&copy; COVER Corp.)
					</td>
					<td align="center">
						<a href="https://www.youtube.com/channel/UC0WwEfE-jOM2rzjpdfhTzZA">Aizono Manami</a> (&copy; Ichikara Inc.)
					</td>
				</tr>
			</table>
			<b>Figure 7.16</b> Two characters with "maro" eyebrows. Instead of moving them, the system found nearby black lines and moved these instead.
		</p>

		<p>The system can also mistake similar or nearby features for facial organs.</p>

		<p>
			<table align="center">
				<tr>
					<td>
					<video muted controls loop width="180">
					<source src="data/characters/suzuhara_lulu/expression_only_video.mp4" type="video/mp4">
					</video>
					</td>
					<td>
					<video muted controls loop width="180">
					<source src="data/characters/sekishiro_miko/expression_only_video.mp4" type="video/mp4">
					</video>
					</td>
					<td>
					<video muted controls loop width="180">
					<source src="data/characters/gwelu_os_gar/expression_only_video.mp4" type="video/mp4">
					</video>
					</td>
				</tr>
				<tr>
					<td align="center">
					<a href="https://www.youtube.com/channel/UCUzJ90o1EjqUbk2pBAy0_aw">Suzuhara Lulu</a><br>(&copy; Ichikara Inc.)
					</td>
					<td align="center">
					<a href="https://www.youtube.com/channel/UCDh2bWI5EDu7PavqwICkVpA">Sekishiro Mico</a><br>(&copy; 774 Inc.)
					</td>
					<td align="center">
					<a href="https://www.youtube.com/channel/UC1QgXt46-GEvtNjEC1paHnw">Gwelu Os Gar</a><br>(&copy; Ichikara Inc.)
					</td>
				</tr>
			</table>
			<b>Figure 7.17</b> My network mistook a strand of Suzuhara's hair ties for her right eyebrow, a part of Sekishiro's eyepatch for her eyelash, and the space between the lines of Os Gar's lip for his mouth.
		</p>

		<p>Lastly, occlusions that are rare in the training dataset can cause problems.</p>

		<p>
			<table align="center">
				<tr>
					<td>
					<video muted controls loop>
					<source src="data/characters/amane_kanata/expression_only_video.mp4" type="video/mp4">
					</video>
					</td>
					<td>
					<video muted controls loop>
					<source src="data/characters/tenkai_tsukasa/expression_only_video.mp4" type="video/mp4">
					</video>
					</td>
				</tr>
				<tr>
					<td align="center">
					<a href="https://www.youtube.com/channel/UCUzJ90o1EjqUbk2pBAy0_aw">Amane Kanata</a> (&copy; COVER Corp.)
					</td>
					<td align="center">
					<a href="https://www.youtube.com/channel/UCZYyhgoV314CM14zBD6vd4A">Tenkai Tsukasa</a> (&copy; Tenkai Tsukasa)
					</td>
				</tr>
			</table>
			<b>Figure 7.18</b> The system could not animate Amane's mouth because her chin is hidden by a black collar, and the sunglasses caused Tenkai's left eye to deform incorrectly. Both cases are examples of uncommon occlusions: almost all models in the training set have their chins fully exposed, and, while top parts of the eyes are often occluded by hair, not many eyes have their bottom parts occluded.
		</p>

		<h3>7.5 &nbsp; Miscellaneous Results</h3>

		<p>One of the strengths of my system (and any parameter-based poser) is its flexibility: one can combine it with any source of pose parameters. I thus used it to create several content creation tools.</p>

		<h4>7.5.1 &nbsp; GUI for Manipulating Character Faces</h4>

		<p>I created a desktop application that lets the user manipulate an anime character's facial expression and face rotation by dragging sliders. The resulting images can be saved for later use.</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/535mjOjpy38" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>
		
		<h4>7.5.2 &nbsp; Animating Drawings with Motions for 3D Models</h4>

		<p>I wrote a program that converts motions for MMD models into sequences of pose vectors, allowing me do drive 2D illustrations with motions intended for 3D models. With it, I created 4 music videos.</p>

		<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/2yqenOki4lI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<br>
		TikTok's "Wink"
		</p>

		<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/mfENtYixnNE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<br>
		Alien Alien
		</p>

		<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/-_bUkcJfggk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<br>
		Ochame Kinou
		</p>

		<p align="center">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/uhx-S5gAX2Q" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<br>
		Otome Kaibou
		</p>

		<h4>7.5.3 &nbsp; Motion Transfer from Facial Performances</h4>

		<p>I created another tool that allows an actor to control a character in real time. I use an iOS application called <a href="https://www.ifacialmocap.com/">iFacialMocap</a> to capture facial performance. It uses iPhone's depth camera and Apple's <a href="https://developer.apple.com/documentation/arkit">ARKit library</a> to estimate blendshape parameters and head rotation angles and streams them to a PC through a UDP connection. I wrote a receiver that convert the signals to pose vectors and feed them to the network in real time, allowing me to have characters mimic my facial movement.</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/m13MLXNwdfY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</p>

		<p>I also recorded myself saying a tongue twister, reading aloud Japanese text, and lip syncing three pieces of music. I transferred the motions to various characters to create the following videos.</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/rLr9Sc0Qd9I" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			Reciting <a href="https://japanese-wiki-corpus.github.io/culture/Uiro%20uri%20(The%20Medicine%20Peddler).html">the Medicine Peddler's Sales Pitch</a> (外郎売)
		</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/JOT1dGacw1U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			Reading aloud <a href="https://en.wikipedia.org/wiki/Ry%C5%ABnosuke_Akutagawa">Akutagawa Ryuunosukera</a>'s "Rail Truck"
		</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/_O5BEcUz3Bw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			Lip syncing <a href="https://yakuza.fandom.com/wiki/Bakamitai">Baka Mitai</a> (the song used in the <a href="https://knowyourmeme.com/memes/dame-da-ne-baka-mitai">Dame Da Ne meme</a>)
		</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/nOMrNqvli_M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			Lip syncing <a href="https://www.youtube.com/watch?v=XsWKqTqS988">Happy Birthday Class Rep Song</a> (委員長おめでとうの歌)
		</p>

		<p align="center">
			<iframe width="560" height="315" src="https://www.youtube.com/embed/l87TDzwhCmk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br>
			Lip syncing <a href="https://www.youtube.com/watch?v=hw8whKP0HdU">GOMIKASU-Original Mix-</a>
		</p>

		<p>I hope I have convinced you through the videos that the characters lip sync and imitate my facial movements well. Moreover, my system preserves the beauty of the illustrations as it does not drastically distorts the faces unlike Siarohin et al.'s system, which is widely used to generate Dame Da Ne meme videos such as the one shown below.</p>		

		<blockquote class="twitter-tweet" align="center"><p lang="en" dir="ltr">you can have it I just got even better idea <a href="https://t.co/LnCkMZK51K">pic.twitter.com/LnCkMZK51K</a></p>&mdash; Cakewalking 5555+1 (@Tortokhod) <a href="https://twitter.com/Tortokhod/status/1284106901001830400?ref_src=twsrc%5Etfw">July 17, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

		<a name="conclusion"></a>
		<h2>8 &nbsp; Conclusion</h2>

		<p>I have presented a new network architecture for changing facial expressions of images of anime characters. It is capable of deforming the eyebrows, the eyelids, the irises, and the mouth, all of which are facial features important for conveying emotions. Possible movements come from common 37 blendshapes and 2 bone joints found in 8,000 downloadable MMD models. They allow characters to express various emotions (Figure 7.12) and perform good lip syncs (Section 7.5.2 and 7.5.3). The whole system compares favorably to two previous works, producing plausible head rotation and consistently higher quality faces. Moreover, it can be combined with any source of pose vectors, allowing me to easily create contents and tools as shown in the last section.</p>

		<p>The main insight of this article is that high quality results require paying attention to specific facial features. To generate crisp eyebrows, I have separate networks segment out, warp, and composite them back into the input image. I also augment my data generation process so that it simulates the possible non-physical layering of the eyebrows and renders them in ways they are drawn on in-the-wild anime characters. Lastly, to generate artifact-free eyelids, I introduce a step to deform them without using appearance flow.</p>

		<p>A major limitation of this article's approach is that the possible movements are limited to the common blendshapes found in MMD models. Hence, it is not yet possible to have anime characters imitate all types of human facial movements.</p>

		<p>This project was born out of my desire to make the 2019 system more practical, and it successfully solves the lack of facial expressiveness. However, many problems remain. The model has become much bigger, disoccluded parts after rotation can use improvement, and many restrictions still exist on the input image. I will address these shortcomings in future projects.</p>

		<h2>9 &nbsp; Disclaimer</h2>

		<p>While I'm an employee of Google Japan, this project is my personal hobby which I did in my free time without using Google's resources. It has nothing to do with work as I am a normal software engineer writing  Google Maps backends for a living. While I did computer graphics research in my previous life, I currently do not belong to any of Google's or Alphabet's research organizations. Opinions expressed in this article is my own and not the company's. Google, though, may claim rights to the article's technical inventions.</p>

		<h2>10 &nbsp; Special Thanks</h2>

		<p>I would like to thank <a href="https://aixile.github.io/">Yanghua Jin</a>, <a href="https://twitter.com/alitaso346">Alice Maruyama</a>, <a href="https://cory.li/">Cory Li</a>, <a href="https://shinjiogaki.github.io/">Shinji Ogaki</a>, <a href="https://ppasupat.github.io/">Panupong Pasupat</a>, <a href="http://jamorn.me">Jamorn Sriwasansak</a>, <a href="https://alantian.net/">Yingtao Tian</a>, Mamoru Uehara, and <a href="https://www.linkedin.com/in/jayakorn-vongkulbhisal-32253849/">Jayakorn Vongkulbhisal</a> for their comments.</p>

		<hr>
		<b>Update History</b>
		<ul>
			<li>2021/02/02: First publication.</li>
		</ul>

		<hr>
		<p align="right"><font size="1">Project <a href="https://ja.wikipedia.org/wiki/%E3%83%96%E3%83%BC%E3%82%B2%E3%83%B3%E3%83%93%E3%83%AA%E3%82%A2">Bougainvillea</a></font></p>
	</div>
    <script src="js/bootstrap.bundle.min.js"></script>    
    <script>
    $.bigfoot();
  	</script>
  </body>  
</html>
